<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Git Flow & Git 命令]]></title>
    <url>%2F2017-06%2FGit_Flow_and_Git_Command.html</url>
    <content type="text"><![CDATA[这篇文章是针对 Git Flow &amp; Git 命令 的 总结。 我花费了很大精力才完成的！ 1. 基本概念1.1 Git V.S. SVNSVN 是 集中化 的版本控制系统, 虽然能够令多个团队成员一起协作开发, 但有时如果中央服务器宕机的话, 谁也无法在宕机期间提交更新和协同开发。甚至有时, 中央服务器磁盘故障, 恰巧又没有做备份或备份没及时, 那就可能有丢失数据的风险。 Git 是 分布式 的版本控制系统, 客户端不只是提取最新版本的快照, 而且将整个代码仓库镜像复制下来。如果任何协同工作用的服务器发生故障了, 也可以用任何一个代码仓库来恢复。而且在协作服务器宕机期间, 你也可以提交代码到本地仓库, 当协作服务器正常工作后, 你再将本地仓库同步到远程仓库。 1.2 功能 &amp; 特性 版本控制 &amp; 多人协作开发 强大的 分支特性, 所以能够灵活地以 不同的工作流 协同开发 分布式版本控制系统, 即使协作服务器宕机, 也能继续提交代码或文件到本地仓库, 当协作服务器恢复正常工作时, 再将本地仓库同步到远程仓库。 直接记录快照, 而非差异比较 有关以上特性的详细解释, 请查看 Pro git 的 git 基础章节 1.3 Git 基本技巧(1) Git 命令别名关于具体如何使用命名别名技巧, 请查看 Pro git 的技巧和窍门 2. Git Flow由于 git 拥有强大的 分支特性, 它的工作流比较灵活而缺乏约束, 于是参考 Atlassian Git Tutorial 的 Comparing Workflows 章节, 在此提供 五种 Git 工作流: 2.1 Basic Workflow (基本工作流)2.2 Centralized Workflow (集中式工作流)2.3 Feature Branch Workflow (功能分支工作流)2.4 Gitflow Workflow (Gitflow 工作流)2.5 Forking Workflow (Forking 工作流) 最后会介绍一个集大成者: 一套 开发流程(分支策略和发布管理) 模型, 先看图眼馋一下。 2.1 Basic Workflow (基本工作流) 在 git 版本控制的目录下修改某个文件 使用 git add 命令对修改后的文件快照, 保存到暂存区域 使用 git commit 命令提交更新, 将保存在暂存区域的文件快照永久转储到 Git 目录中 2.2 Centralized Workflow (集中式工作流)过渡到分布式版本控制系统看起来像一个艰巨的任务, 但如果你充分利用好 git 的话, 你不必改变你既有的工作流, 你的团队可以采用与之前使用 SVN 一样的方式来开发项目。 (1) 如何工作 从远程仓库 (central repository) 克隆工程到本地仓库 (local repository) — git clone 在本地仓库编辑文件和提交更新 — git add 和 git commit fetch 远程仓库已更新的 commit 到本地仓库和 rebase 到已更新的 commit 的上面 —git fetch 和 git rebase 或 git pull --rebase push 本地主分支 (master branch) 到远程仓库 — git push (2) 管理冲突 何时发生冲突: 在开发者发布它们功能之前, 他们需要 fetch 远程仓库已更新的 commit 到本地仓库和 rebase 到已更新的 commit 的上面。有时, 本地提交与远程提交会发生冲突, git 会暂停 rebase 过程来让你手动解决冲突。 如何解决冲突: 你可以使用 git status 和 git add 来手动解决合并时冲突。 2.3 Feature Branch Workflow (功能分支工作流)Feature Branch Workflow 的主要思想就是在 开发每个功能 时都应该创建 一个独立的分支 而不只是使用主分支。由于每个分支是独立且互不影响, 这就意味着主分支不会包含 broken code, 对持续集成环境是很有帮助的。 (1) 如何工作 仍然使用远程仓库 (central repository) 和主分支 (master branch) 记录官方工程的历史 开发者每次开发新功能时都创建一个新分支 — git checkout -b Feature branches 应该推送到远程仓库 (central repository) — git push 发送 pull request 来请求管理员能否合并到主分支 (master branch) 发布新功能到远程仓库 (central repository) (2) Pull RequestPull request 是一种当开发者完成一个新功能后向其他团队成员发送通知的机制。它的使用过程如下: 开发者可以通过 Github 或 Bitbucket 发送 pull request 其他的团队成员审查、讨论和修改代码 项目维护者合并新增功能分支到主分支 (master branch), 然后关闭 pull request 2.4 Gitflow Workflow (Gitflow 工作流)Feature Branch Workflow 是一种非常灵活的开发方式。对于一些规模比较大的团队, 最好就是给特定的分支赋予不同的角色。除了 功能分支 (feature branch), Gitflow Workflow 还使用独立的分支来 准备发布 (preparing)、维护 (maintaining) 和 记录版本 (recording releases)。下面我会逐个介绍这个几个分支: Historical Branches、Feature Branches、Release Branches 和 Maintenance Branches。 (1) Historical Branches master 分支 保存官方发布历史 develop 分支 衍生出各个 feature 分支 (2) Feature Branches feature 分支 使用 develop 分支作为它们的父类分支 当其中一个 feature 分支完成后, 它会合并会 develop 分支 feature 分支应该从不与 master 分支直接交互 (3) Release Branches release 分支 主要用来清理释放、测试和更新文档 一旦 develop 分支获得足够的功能来发布时, 你可以从 develop 衍生出一个 release 分支 一旦准备好上架, release 合并到 master 分支并且标记一个版本号 另外, 还需要合并回 develop 分支 (4) Maintenance Branches maintenance 分支 用来快速给已发布产品修复 bug 或微调功能 它从 master 分支直接衍生出来 一旦完成修复 bug, 它应该合并回 master 分支和 develop 分支 master 应该被标记一个新的版本号 (5) 标记 Tags使用两个命令来给 master 分支标记版本号: git tag -a 0.1 -m &quot;Initial public release&quot; master git push origin master --tags 2.5 Forking Workflow (Forking 工作流)Forking Workflow 与以上讨论的工作流很不同, 一个很重要的 区别 就是它不只是多个开发共享一个远程仓库 (central repository), 而是每个开发者都拥有一个独立的服务端仓库。也就是说每个 contributor 都有两个仓库: 自己的私有的远程仓库和官方的共享的远程仓库。 Forking Workflow 这种工作流主要好处就是每个开发者都拥有自己的远程仓库, 可以将提交的 commits 推送到自己的远程仓库, 但只有工程维护者才有权限 push 提交的 commits 到官方的仓库, 其他开发者在没有授权的情况下不能 push。Github 很多 开源项目 都是采用 Forking Workflow 工作流。 (1) 如何工作 在服务器上有一个官方公共的仓库 开发者 fork 官方仓库来创建它的拷贝, 然后存放在服务器上 当开发者准备好发布本地的 commit 时, 他们 push commit 到他们自己的公共仓库 在自己的公共仓库发送一个 pull request 到官方仓库 维护者 pull 贡献者的 commit 到他自己的本地仓库 审查代码确保它不会破坏工程, 合并它到本地仓库的 master 分支 push master 分支到服务器上的官方仓库 其他开发者应该同步官方仓库。 2.6 终极开发流程 (分支策略和发布管理) 模型先上图。 本节全部来自这篇文章: 一个成功的 Git 分支模型, 原文与译文都不错, 我就不转载了, 直接去看吧。 3. Git 命令清单一般来说, 日常使用只要记住下图 6 个命令, 就可以了。但是熟练使用, 恐怕要记住 60～100 个命令。 下面是一份常用 Git 命令清单。几个专用名词的译名如下。 Workspace: 工作区 Index / Stage: 暂存区 Repository: 本地仓库 Remote: 远程仓库 你的本地仓库由 git 维护的三棵 “树” 组成。第一个是你的 工作目录, 它持有实际文件; 第二个是 暂存区(Index/Stage), 它像个缓存区域, 临时保存你的改动; 最后是 HEAD, 它指向你最后一次提交的结果。 文件状态 未跟踪 Untracked 已跟踪 Tracked 已修改 modified 已放入暂存区 staged 未修改 committed 3.1 配置与帮助 config helpGit 的设置文件为 .gitconfig, 它可以在用户主目录下(全局配置), 也可以在项目目录下(项目配置)。 12345678910111213141516171819202122232425# 显示当前的 Git 配置$ git config --list# 编辑 Git 配置文件$ git config -e [--global]# 设置提交代码时的用户信息$ git config [--global] user.name "[name]"$ git config [--global] user.email "[email address]"# 其它的一些重要配置$ git config [--global] core.quotepath false # 使 git 命令输出中的中文正确显示$ git config [--global] core.editor$ git config [--global] diff.tool$ git config [--global] merge.tool$ git config [--global] help.format# 设置彩色输出, 一般默认就是$ git config [--global] color.ui true# 显示帮助信息# -i: info 形式展示 -m: man 形式展示 -w: web(html)形式展示。windows 的 git bash 没有 man, 只能 web 形式展示$ git help$ git help [command]$ git [command] --help 3.2 新建代码库 init clone12345678910111213141516# 将当前目录初始化为 Git 代码库$ git init# 在当前目录新建一个目录, 并将其初始化为 Git 代码库$ git init [project-name]# 创建远端仓库的克隆 [指定目录名]$ git clone [url]$ git clone [url] [dir-name]# 指定远程主机名(默认 origin)$ git clone -o [remote-name] [url]# 创建本地仓库的克隆 [指定目录名]$ git clone /path/to/repository$ git clone /path/to/repository [dir-name] 3.3 增加 / 删除 / 移动文件 add rm mv1234567891011121314151617181920# 开始跟踪文件并放到暂存区 或 把已跟踪文件放到暂存区 或 合并时把有冲突的文件标记为已解决状态$ git add [file1] [file2] ...# 添加指定目录到暂存区, 包括子目录$ git add [dir]# 添加当前目录的所有文件到暂存区$ git add .# 对每个变化都会要求确认是否 Stage, 对于同一个文件的多处变化, 可以实现部分提交、分次提交$ git add -p# 停止追踪指定文件, 但该文件会保留在工作区$ git rm --cached [file]# 删除工作区文件, 并且将这次删除放入暂存区$ git rm [file1] [file2] ...# 改名文件, 并且将这个改名放入暂存区$ git mv [file-original] [file-renamed] 3.4 提交 commit1234567891011121314151617# 提交暂存区到仓库区$ git commit -m [message]# 提交暂存区的指定文件到仓库区$ git commit [file1] [file2] ... -m [message]# 提交工作区自上次 commit 之后的变化, 直接到仓库区(省下 git add 操作)(但是只有已追踪文件的更改会提交, 未追踪文件还是要先 git add)$ git commit -a -m [message]# 提交时显示所有 diff 信息$ git commit -v# 用一次新的提交(新的 commit id), 重写最近一次提交(并包括所有新变化)。如果没有文件更改, 也可以用来改写上一次 commit 的提交信息$ git commit --amend -m [message]# 重写上一次 commit, 并包括指定文件的新变化$ git commit --amend [file1] [file2] ... -m [message] 3.5 分支 branch checkout merge rebase cherry-pick123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081# 列出所有本地分支$ git branch# 列出所有远程分支, 远程分支: 除了 Git 不允许你检出 (切换到该分支) 之外, 远程分支跟本地分支没区别 --- 你可以将它们合并到当前分支(merge、rebase), 与其他分支作比较差异(diff), 查看那些分支的历史日志(log), 等等。PS: 在本地主机上要用 "远程主机名/分支名" 的形式访问。$ git branch -r# 列出所有本地分支和远程分支$ git branch -a* master dev-1 remotes/origin/master remotes/origin/dev-1# 表示, 本地主机的当前分支是 master, 远程分支有 origin/master origin/dev-1# 新建一个分支(以当前分支为基础), 但依然停留在当前分支$ git branch [new-branch-name]# 新建一个分支(以当前分支为基础), 并切换到该分支$ git checkout -b [new-branch-name]# 新建一个分支(以指定的本地分支为基础)$ git branch [new-branch-name] [branch-name]$ git checkout -b [new-branch-name] [branch-name]# 新建一个分支(以指定的远程分支为基础, 会自动建立追溯关系)$ git branch [new-branch-name] [origin/master]$ git checkout -b [new-branch-name] [origin/master]# 新建一个分支, 以指定 commit 为基础$ git branch [new-branch-name] [commit]$ git checkout -b [new-branch-name] [commit]# 将现有分支与指定的远程分支建立追踪关系$ git branch --set-upstream [branch-name] [remote-branch]# 创建一个空白分支: 1. 创建一个 orphan 的分支, 这个分支是独立的 2. 删除原来代码树下的所有文件$ git checkout --orphan [branch-name]$ git rm -rf .# 注意这个时候你用 git branch 命令是看不见当前分支的名字的(无任何提交的分支都看不见), 除非你进行了第一次 commit# 切换到指定分支, 并更新工作区(未提交的就没了! 解决方案见 git stash)$ git checkout [branch-name]# 切换到上一个分支$ git checkout -# 删除本地分支$ git branch -d [branch-name]# 删除远程分支$ git push [remote-name] --delete [remote-branch-name]$ git push [remote-name] :[remote-branch-name]$ git branch -dr [remote-name/remote-branch-name]# 合并指定本地分支/远程分支 (要用 "远程主机名/分支名" 的形式访问) 到当前分支$ git merge [branch-name]$ git merge [remote-name/branch-name]# 左: 非快进式合并(会多一个节点) 右: 快进式合并## * 2d3acf9 **** * 2d3acf9 ****# * 5e3ee11 Merge branch **** |# |\ |# | * 420eac9 **** * 420eac9 ****# | * 30e367c **** * 30e367c ****# | * 5a09431 **** * 5a09431 ****# | * e1193f8 **** * e1193f8 ****# |/ |# * d6016bc **** * d6016bc ****# * 11d191e **** * 11d191e ****## 非快进式合并可以刻意地弄出提交线图分叉(如上图: 本可以合并为一条提交线), 更清晰地告诉你同伴: 这一系列的提交都是为了实现同一个目的(一般是从 feature 分支合并过来的)# 默认情况下, Git 执行 "快进式合并"(fast-farward merge), 使用 --no-ff 选项进行非快进式合并$ git merge --no-ff [branch-name]# $ git rebase# 选择一个(别的分支中的) commit, 合并到当前分支$ git cherry-pick [commit] 3.6 远程 fetch remote pull push123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 获取远程仓库的变动, 通常用来查看其他人的进程, 因为它取回的代码对你本地的开发代码没有影响, 取回的代码, 在本地主机上要用 "远程主机名/分支名" 的形式访问, 如 git reset --hard origin/master, git checkout -b dev-1 origin/master# git fetch [remote-name] [remote-branch-name] + git merge [remote-name/remote-branch-name] = git pull [remote-name] [remote-branch-name]。所以一般不会直接使用 fetch$ git fetch [remote-name]$ git fetch [remote-name] [branch-name]# 显示所有远程仓库, 带远程主机网址$ git remote -v# 显示某个远程仓库的信息$ git remote show [remote-name]# 将你的仓库连接到某个远程仓库, 并命名(clone 下来的远程仓库默认名字是 origin)$ git remote add [remote-name] [url]# 远程仓库的重命名$ git remote rename [remote-name-old] [remote-name-new]# 远程仓库的移除$ git remote rm [remote-name]# 取回远程主机某个分支的更新并与本地分支合并, git pull = git fetch + git merge, 完整格式如下$ git pull [remote-name] [remote-branch-name]:[local-branch-name]# 与当前分支合并, 则冒号后面的部分可以省略$ git pull [remote-name] [remote-branch-name]# 如果当前分支与远程分支存在追踪关系, git pull 就可以省略远程分支名$ git pull [remote-name]# 如果当前分支只有一个追踪分支, 连远程主机名都可以省略$ git pull# 如果合并需要采用 rebase 模式, 可以使用 --rebase 选项 = git fetch + git rebase$ git pull --rebase [remote-name] [remote-branch-name]:[local-branch-name]# 将本地分支的提交, 推送到远程主机, 完整格式如下, 注意与 pull 顺序不一样$ git push [remote-name] [local-branch-name]:[remote-branch-name]# 如果省略远程分支名, 则表示将本地分支推送与之存在 "追踪关系" 的远程分支(通常两者同名), 如果该远程分支不存在, 则会被新建$ git push [remote-name] [local-branch-name]# 如果省略本地分支名, 则表示删除指定的远程分支, 因为这等同于推送一个空的本地分支到远程分支。$ git push [remote-name] :[remote-branch-name]# 等同于$ git push [remote-name] --delete [remote-branch-name]# 如果当前分支与远程分支之间存在追踪关系, 则本地分支和远程分支都可以省略。$ git push [remote-name]# 如果当前分支只有一个追踪分支, 那么主机名都可以省略。$ git push# 如果当前分支与多个主机存在追踪关系, 则可以使用 -u 选项指定一个默认主机, 这样后面就可以不加任何参数使用 git push。$ git push -u [remote-name] [remote-branch-name]# 强行推送当前分支到远程仓库, 即使有冲突(慎重!!)$ git push [remote-name] --force# 用途: 撤销远程仓库的 commit 记录git reset --hard HEAD^git push -f# 将本地的所有分支都推送到远程主机, 使用 --all 选项$ git push [remote-name] --all# git push 不会推送标签(tag), 必须使用 --tags 选项单独推送标签$ git push [remote-name] --tags 3.7 查看信息 status log shortlog diff show reflog blame bisect123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120# 显示工作区和暂存区的状态$ git status# 显示当前分支的版本历史$ git log# 常用选项及其释义# -&lt;n&gt; 显示最近 n 条提交# --stat 显示每次更新的文件修改统计信息。# --shortstat 只显示 --stat 中最后的行数修改添加移除统计。# -p 按补丁格式显示每个更新之间的差异。# --word-diff 按 word diff 格式显示差异。# --name-only 仅在提交信息后显示已修改的文件清单。# --name-status 显示新增、修改、删除的文件清单。# --abbrev-commit 仅显示 SHA-1 的前几个字符, 而非所有的 40 个字符。# --relative-date 使用较短的相对时间显示(比如, "2 weeks ago")。# --pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline, short, full, fuller 和 format(后跟指定格式)。# --oneline --pretty=oneline --abbrev-commit 的简化用法。# --graph 显示 ASCII 图形表示的分支合并历史。## 还有一些用于筛选的选项, 见: https://git-scm.com/book/zh/v1/Git-基础-查看提交历史#限制输出长度# --since, --after 仅显示指定时间之后的提交。# --until, --before 仅显示指定时间之前的提交。# --author 仅显示指定作者相关的提交。# --committer 仅显示指定提交者相关的提交。# gitk: git log 命令的可视化版本, 凡是 git log 可以用的选项也都能用在 gitk 上。在项目工作目录中输入 gitk 命令后, 即可启动# 显示过去 5 次提交$ git log -5# 显示 commit 历史, 以及每次 commit 变更的简略统计信息$ git log --stat# 使用不同于默认格式的方式展示提交历史 oneline short full fuller$ git log --oneline # 只有 oneline 可以这样用$ git log --pretty=oneline$ git log --pretty=short$ git log --pretty=full$ git log --pretty=fuller# 还可以用 format 来定制要显示的记录格式, 便于后期编程提取分析, 见: https://git-scm.com/book/zh/v2/Git-基础-查看提交历史#pretty_format# 用 oneline 或 format 时结合 --graph 选项, 可以看到用 ASCII 字符串表示的简单图形, 形象地展示了每个提交所在的分支及其分化衍合情况。例如$ git log --pretty=format:"%h %s" --graph* 2d3acf9 ignore errors from SIGCHLD on trap* 5e3ee11 Merge branch 'master' of git://github.com/dustin/grit|\| * 420eac9 Added a method for getting the current branch.* | 30e367c timeout code and tests* | 5a09431 add timeout protection to grit* | e1193f8 support for heads with slashes in them|/* d6016bc require time for xmlschema* 11d191e Merge branch 'defunkt' into local# 显示每次提交的 diff$ git log -p# 显示指定文件每一次变更的 diff$ git log -p [file]# 显示某个用户的所有提交$ git log --author="username"# 显示指定提交者相关的提交$ git log --committer="committername"# since, after, until, before 用法一样$ git log --since="2008-10-01" # 2008-10-01 之后的提交$ git log --since=2.weeks # 近两周的提交$ git log --since="2 years 1 day 3 minutes ago" # 2 年 1 天 3 分钟之前 至 今 的提交# 搜索提交历史, 根据关键词$ git log -S [keyword]# 显示某个 commit 之前的变动$ git log [commit]$ git log [commit] --pretty=format:"%h %s"# 显示某个文件的版本历史, 包括文件改名$ git log --follow [file]$ git whatchanged [file]# git shortlog 是一种特殊的 git log, 它是为创建发布声明设计的。它把每个提交按作者分类, 显示提交信息的第一行。这样可以容易地看到谁做了什么。默认情况下按作者名字排序, 但你可以传入 -n 选项来按每个作者提交数量排序。-s 选项不显示提交描述, 只显示提交数$ git shortlog -sn# 显示暂存区和工作区 (已追踪文件) 的差异$ git diff# 显示暂存区与当前分支最新 commit 之间的差异$ git diff --staged$ git diff --staged [file]# 显示工作区与当前分支最新 commit 之间的差异$ git diff HEAD# 显示两个分支 (的最新版本) 之间的差异$ git diff [first-branch] [second-branch]# 显示两个 commit 之间的差异$ git diff [first-commit-id] [second-commit-id]# "git diff A...B" is equivalent to "git diff $(git-merge-base A B) B". 展示从 A B 最近共同祖先到 B 的 diff$ git diff [first-commit-id]...[second-commit-id]# 显示今天你写了多少行代码$ git diff --shortstat "@&#123;0 day ago&#125;"# 显示某次提交的元数据和内容变化$ git show [commit]# 显示某次提交发生变化的文件$ git show --name-only [commit]# 显示某次提交时, 某个文件的内容$ git show [commit]:[filename]# 如不小心用 git commit --amend 当成 git commit 覆盖当前的 commit, 或不小心把当前的 commit 给搞没了(reset --hard), 可以通过引用日志 reflog 命令来了解你对 HEAD 值的改变(或用 git log -g 以标准日志的格式输出引用日志), 然后通过 git reset 命令进行恢复$ git reflog# blame 和 bisect 命令参见: https://git-scm.com/book/zh/v2/Git-工具-使用-Git-调试# 显示 指定文件的各部分 最后一次修改的信息: 什么人在什么时间修改过$ git blame [file] 3.8 撤销 checkout reset revert clean stash常用 checkout 来撤销未暂存的修改, reset HEAD 来撤销没有提交的更改, revert 来撤销已经提交的更改。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# 1. git reset 将一个分支的 HEAD 指针指向另一个提交, 这可以用来移除当前分支的一些提交。下次 Git 执行垃圾回收 (很久很久一次) 的时候, 这两个提交会被删除(在此之前用 git reflog 找到丢掉的 commit id, 再用 git reset 可以还原回来)。换句话说, 如果你想扔掉这两个提交, 就用 git reset。除了在当前分支上操作, 你还可以通过传入这些标记来修改你的缓存区或工作目录# --soft 缓存区和工作目录都不会被改变# --mixed 默认选项。缓存区和你指定的提交同步, 但工作目录不受影响# --hard 缓存区和工作目录都同步到你指定的提交# 2. git checkout 与 git reset 类似, 但除了 `git checkout [file]` 和 `git checkout .` 外不推荐使用 checkout 来做撤销, checkout 用于分支切换比较好# 注: git reset 和 git checkout 命令也接受文件路径作为参数。这时它的行为就大为不同了。它不会作用于整份提交, 参数将它限制于特定文件。--soft 和 --hard 对文件层面的 reset 毫无作用, 因为缓存区中的文件一定会变化, 而工作目录中的文件一定不变(即 --mixed)。# 3. git revert 通过创建一个新的提交来抵消一个提交的更改。这是一个安全的方法, 因为它不会重写提交历史。因此, revert 可以用在公共分支上, reset 应该用在私有分支上。# 恢复暂存区的指定文件到工作区(常用)$ git checkout [file]# 恢复暂存区的所有文件到工作区(常用)$ git checkout .# 恢复某个 commit 的所有文件到暂存区和工作区$ git checkout [commit]# 恢复某个 commit 的指定文件到暂存区和工作区$ git checkout [commit] [file]# 重置暂存区的指定文件, 与上一次 commit 保持一致, 但工作区不变$ git reset [file]# 重置当前分支的 HEAD 指针为指定 commit, 同时重置暂存区, 但工作区不变$ git reset [commit]# 重置暂存区与工作区, 与上一次 commit 保持一致(常用)$ git reset --hard# 重置当前分支的 HEAD 指针为指定 commit, 同时重置暂存区和工作区与指定 commit 一致(常用)$ git reset --hard [commit]# 重置当前 HEAD 为指定 commit, 但保持暂存区和工作区不变(常用)$ git reset --keep [commit]# 新建一个 commit, 用来撤销指定 commit, 后者的所有变化都将被前者抵消, 并且应用到当前分支(常用)$ git revert [commit]# git clean 命令用来从你的工作目录中删除所有没有 tracked 过的文件。git clean 经常和 git reset --hard 一起结合使用, 因为 reset 只影响被 track 过的文件, 所以需要 clean 来删除没有 track 过的文件, 结合使用这两个命令能让你的工作目录完全回到一个指定的 [commit] 的状态。git clean 对于刚编译过的项目也非常有用。如, 他能轻易删除掉编译后生成的.o 和.exe 等文件。这个在打包要发布一个 release 的时候非常有用。# -n 选项是一次 clean 的演习, 告诉你哪些文件会被删除。记住他不会真正的删除文件, 只是一个提醒$ git clean -n# 删除当前目录下所有没有 track 过的文件。他不会删除 .gitignore 文件里面指定的文件夹和文件, 不管这些文件有没有被 track 过git clean -f# 删除指定路径下的没有被 track 过的文件git clean -f &lt;path&gt;# 删除当前目录下没有被 track 过的文件和文件夹git clean -df# 当你的工作区不 clean, 而你想转到其他分支上进行一些工作, 却不想提交进行了一半的工作, 可以用 git stash 命令, 它命令可以把当前工作现场 "储藏" 起来, 等以后恢复现场后继续工作。使用 git stash 命令后再用 git status 查看工作区, 就是 clean 的, 可以方便地切换到其他分支工作, 而你的变更都保存在栈上。甚至可以在其中一个分支上保留一份储藏, 切换到另外一个分支, 再重新应用这些变更(有点像 rebase)。$ git stash# 可以加上信息$ git stash save "message..."# 查看现有的储藏$ git stash list# 重新应用你刚刚实施的储藏$ git stash apply# 应用更早的储藏, 你可以通过名字指定它, 如果你不指明, Git 默认使用最近的储藏并尝试应用它$ git stash apply stash@&#123;0&#125;# apply 选项只尝试应用储藏的工作 --- 储藏的内容仍然在栈上。要移除它, 你可以运行 git stash drop, 加上你希望移除的储藏的名字$ git stash drop stash@&#123;0&#125;# 可以运行 git stash pop 来重新应用储藏, 同时立刻将其从堆栈中移走$ git stash pop# 清空储藏栈$ git stash clear 3.9 标签 tag1234567891011121314151617181920212223242526# 列出所有 tag$ git tag# 新建一个 tag 在当前 commit$ git tag [tag]# 新建一个 tag 在指定 commit$ git tag [tag] [commit]# 删除本地 tag$ git tag -d [tag]# 删除远程 tag$ git push origin :refs/tags/[tagName]# 查看 tag 信息$ git show [tag]# 提交指定 tag$ git push [remote-name] [tag]# 提交(包含 tag)$ git push [remote-name] --tags# 新建一个分支, 指向某个 tag$ git checkout -b [branch-name] [tag] 3.10 其他12# 生成一个可供发布的压缩包$ git archive 4 高级技巧4.1 修改作者时间 / 提交时间12345678910# 设置作者时间, PS: 很奇怪的一个问题, 月日必须反过来, date -d "Jun 8 14:00 2017" 都不行, 提交后在 Git 上是 Aug 6$ git commit --date="`date -d "2017-08-06 16:21:45"`" -m [message]# 设置提交时间及作者时间(时间问题同上) --- !!!GIT_COMMITTER_DATE="`date -d "2017-08-06 16:21:45"`" git commit --date="`date -d "2017-08-06 16:21:45"`" -m [message]# 事后修改时间(时间问题同上) -- 记得把被重写的 commit 彻底删掉 --- !!!GIT_COMMITTER_DATE="`date -d "2017-08-06 16:21:45"`" git commit --amend --date="`date -d "2017-08-06 16:21:45"`" -m [message]git refloggit reflog delete HEAD@&#123;1&#125; # HEAD@&#123;1&#125;是那个想要删除的 ref 4.2 数据清理与数据维护https://git-scm.com/book/zh/v1/Git-内部原理-维护及数据恢复 4.3 GitHub 技巧在 GitHub 网页任意界面, 按 Shift + / 显示当前可用快捷键。 参考 Git 官方英文文档 Pro Git 简体中文版 Tower 的中文 git 教程 Git 版本控制与工作流 一个成功的 Git 分支模型 常用 Git 命令清单 高质量的 Git 中文教程]]></content>
      <categories>
        <category>版本管理</category>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Git flow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TimSort]]></title>
    <url>%2F2017-04%2FTimSort.html</url>
    <content type="text"><![CDATA[参考: http://www.freebuf.com/vuls/62129.html http://blog.csdn.net/yangzhongblog/article/details/8184707 1Timsort 是结合了合并排序 (merge sort) 和插入排序 (insertion sort) 而得出的排序算法, Tim Peters 在 2002 年 (相比其他排序算法算是后起之秀) 设计了该算法并在 Python 中使用(TimSort 是 Python 中 list.sort 的默认实现), 现在 Java SE7 和 Android 也采用 Timsort 算法对数组排序。 我们评价一个排序算法的好坏要从许多方面衡量, 如下面这张图, 它在现实中有很好的效率。快排虽然平均时间复杂度非常好, 但是在最优、最坏时间复杂度以及算法的稳定性上来说都不如 Timsort。 算法比较]]></content>
      <categories>
        <category>算法</category>
        <category>排序</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 8 新特性探究 之 深入解析日期和时间 - JSR310]]></title>
    <url>%2F2017-04%2FDate-And-Time-Of-Java.html</url>
    <content type="text"><![CDATA[转自: https://my.oschina.net/benhaile/blog/193956 摘要: JSR310 已在 java8 中实现了, 对于恶心的 java.util.Date 和 Calendar, 终于可以休息了吧。。。 这次给大家讲下 java8 时间与日期 API。 众所周知, 日期是商业逻辑计算一个关键的部分, 任何企业应用程序都需要处理时间问题。应用程序需要知道当前的时间点和下一个时间点, 有时它们还必须计算这两个时间点之间的路径。但 java 之前的日期做法太令人恶心了, 我们先来吐槽一下 吐槽 java.util.Date 跟 CalendarTiago Fernandez 做过一次投票, 选举最烂的 JAVA API, 排第一的 EJB2.X, 第二的就是日期 API。 槽点一最开始的时候, Date 既要承载日期信息, 又要做日期之间的转换, 还要做不同日期格式的显示, 职责较繁杂(不懂单一职责, 你妈妈知道吗？纯属恶搞~ 哈哈) 后来从 JDK 1.1 开始, 这三项职责分开了: 使用 Calendar 类实现日期和时间字段之间转换; 使用 DateFormat 类来格式化和分析日期字符串; 而 Date 只用来承载日期和时间信息。 原有 Date 中的相应方法已废弃。不过, 无论是 Date, 还是 Calendar, 都用着太不方便了, 这是 API 没有设计好的地方。 槽点二坑爹的 year 和 month 123Date date = new Date(2012,1,1);System.out.println(date);// 输出 Thu Feb 01 00:00:00 CST 3912 观察输出结果, year 是 2012+1900, 而 month, 月份参数我不是给了 1 吗？怎么输出二月 (Feb) 了？ 应该曾有人告诉你, 如果你要设置日期, 应该使用 java.util.Calendar, 像这样… 12Calendar calendar = Calendar.getInstance();calendar.set(2013, 8, 2); 这样写又不对了, calendar 的 month 也是从 0 开始的, 表达 8 月份应该用 7 这个数字, 要么就干脆用枚举 1calendar.set(2013, Calendar.AUGUST, 2); 注意上面的代码, Calendar 年份的传值不需要减去 1900(当然月份的定义和 Date 还是一样), 这种不一致真是让人抓狂！ 有些人可能知道, Calendar 相关的 API 是 IBM 捐出去的, 所以才导致不一致。 槽点三java.util.Date 与 java.util.Calendar 中的所有属性都是可变的 下面的代码, 计算两个日期之间的天数…. 12345678910111213141516public static void main(String[] args) &#123; Calendar birth = Calendar.getInstance(); birth.set(1975, Calendar.MAY, 26); Calendar now = Calendar.getInstance(); System.out.println(daysBetween(birth, now)); System.out.println(daysBetween(birth, now)); // 显示 0？ &#125;public static long daysBetween(Calendar begin, Calendar end) &#123; long daysBetween = 0; while(begin.before(end)) &#123; begin.add(Calendar.DAY_OF_MONTH, 1); daysBetween++; &#125; return daysBetween;&#125; daysBetween 有点问题, 如果连续计算两个 Date 实例的话, 第二次会取得 0, 因为 Calendar 状态是可变的, 考虑到重复计算的场合, 最好复制一个新的 Calendar 123456789public static long daysBetween(Calendar begin, Calendar end) &#123; Calendar calendar = (Calendar) begin.clone(); // 复制 long daysBetween = 0; while(calendar.before(end)) &#123; calendar.add(Calendar.DAY_OF_MONTH, 1); daysBetween++; &#125; return daysBetween;&#125; JSR310以上种种, 导致目前有些第三方的 java 日期库诞生, 比如广泛使用的 JODA-TIME, 还有 Date4j 等, 虽然第三方库已经足够强大, 好用, 但还是有兼容问题的, 比如标准的 JSF 日期转换器与 joda-time API 就不兼容, 你需要编写自己的转换器, 所以标准的 API 还是必须的, 于是就有了 JSR310。 JSR 310 实际上有两个日期概念。第一个是 Instant, 它大致对应于 java.util.Date 类, 因为它代表了一个确定的时间点, 即相对于标准 Java 纪元 (1970 年 1 月 1 日) 的偏移量; 但与 java.util.Date 类不同的是其精确到了纳秒级别。 第二个对应于人类自身的观念, 比如 LocalDate 和 LocalTime。他们代表了一般的时区概念, 要么是日期(不包含时间), 要么是时间(不包含日期), 类似于 java.sql 的表示方式。此外, 还有一个 MonthDay, 它可以存储某人的生日(不包含年份)。每个类都在内部存储正确的数据而不是像 java.util.Date 那样利用午夜 12 点来区分日期, 利用 1970-01-01 来表示时间。 目前 Java8 已经实现了 JSR310 的全部内容。新增了 java.time 包定义的类表示了日期 - 时间概念的规则, 包括 instants, durations, dates, times, time-zones and periods。这些都是基于 ISO 日历系统, 它又是遵循 Gregorian 规则的。最重要的一点是值不可变, 且线程安全, 通过下面一张图, 我们快速看下 java.time 包下的一些主要的类的值的格式, 方便理解。 方法概览该包的 API 提供了大量相关的方法, 这些方法一般有一致的方法前缀: of: 静态工厂方法。 parse: 静态工厂方法, 关注于解析。 get: 获取某些东西的值。 is: 检查某些东西的是否是 true。 with: 不可变的 setter 等价物。 plus: 加一些量到某个对象。 minus: 从某个对象减去一些量。 to: 转换到另一个类型。 at: 把这个对象与另一个对象组合起来, 例如: date.atTime(time)。 与旧的 API 对应关系 简单使用 java.time 的 API参考 http://jinnianshilongnian.iteye.com/blog/1994164, 被我揉在一起, 可读性很差, 相应的代码都有注释了, 我就不过多解释了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125public class TimeIntroduction &#123; public static void testClock() throws InterruptedException &#123; // 时钟提供给我们用于访问某个特定 时区的 瞬时时间、日期 和 时间的。 Clock c1 = Clock.systemUTC(); // 系统默认 UTC 时钟(当前瞬时时间 System.currentTimeMillis()) System.out.println(c1.millis()); // 每次调用将返回当前瞬时时间(UTC) Clock c2 = Clock.systemDefaultZone(); // 系统默认时区时钟(当前瞬时时间) Clock c31 = Clock.system(ZoneId.of("Europe/Paris")); // 巴黎时区 System.out.println(c31.millis()); // 每次调用将返回当前瞬时时间(UTC) Clock c32 = Clock.system(ZoneId.of("Asia/Shanghai"));// 上海时区 System.out.println(c32.millis());// 每次调用将返回当前瞬时时间(UTC) Clock c4 = Clock.fixed(Instant.now(), ZoneId.of("Asia/Shanghai"));// 固定上海时区时钟 System.out.println(c4.millis()); Thread.sleep(1000); System.out.println(c4.millis()); // 不变 即时钟时钟在那一个点不动 Clock c5 = Clock.offset(c1, Duration.ofSeconds(2)); // 相对于系统默认时钟两秒的时钟 System.out.println(c1.millis()); System.out.println(c5.millis()); &#125; public static void testInstant() &#123; // 瞬时时间 相当于以前的 System.currentTimeMillis() Instant instant1 = Instant.now(); System.out.println(instant1.getEpochSecond());// 精确到秒 得到相对于 1970-01-01 00:00:00 UTC 的一个时间 System.out.println(instant1.toEpochMilli()); // 精确到毫秒 Clock clock1 = Clock.systemUTC(); // 获取系统 UTC 默认时钟 Instant instant2 = Instant.now(clock1);// 得到时钟的瞬时时间 System.out.println(instant2.toEpochMilli()); Clock clock2 = Clock.fixed(instant1, ZoneId.systemDefault()); // 固定瞬时时间时钟 Instant instant3 = Instant.now(clock2);// 得到时钟的瞬时时间 System.out.println(instant3.toEpochMilli());//equals instant1 &#125; public static void testLocalDateTime() &#123; // 使用默认时区时钟瞬时时间创建 Clock.systemDefaultZone() --&gt; 即相对于 ZoneId.systemDefault()默认时区 LocalDateTime now = LocalDateTime.now(); System.out.println(now); // 自定义时区 LocalDateTime now2 = LocalDateTime.now(ZoneId.of("Europe/Paris")); System.out.println(now2);// 会以相应的时区显示日期 // 自定义时钟 Clock clock = Clock.system(ZoneId.of("Asia/Dhaka")); LocalDateTime now3 = LocalDateTime.now(clock); System.out.println(now3);// 会以相应的时区显示日期 // 不需要写什么相对时间 如 java.util.Date 年是相对于 1900 月是从 0 开始 //2013-12-31 23:59 LocalDateTime d1 = LocalDateTime.of(2013, 12, 31, 23, 59); // 年月日 时分秒 纳秒 LocalDateTime d2 = LocalDateTime.of(2013, 12, 31, 23, 59, 59, 11); // 使用瞬时时间 + 时区 Instant instant = Instant.now(); LocalDateTime d3 = LocalDateTime.ofInstant(Instant.now(), ZoneId.systemDefault()); System.out.println(d3); // 解析 String---&gt;LocalDateTime LocalDateTime d4 = LocalDateTime.parse("2013-12-31T23:59"); System.out.println(d4); LocalDateTime d5 = LocalDateTime.parse("2013-12-31T23:59:59.999");// 999 毫秒 等价于 999000000 纳秒 System.out.println(d5); // 使用 DateTimeFormatter API 解析 和 格式化 DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy/MM/dd HH:mm:ss"); LocalDateTime d6 = LocalDateTime.parse("2013/12/31 23:59:59", formatter); System.out.println(formatter.format(d6)); // 时间获取 System.out.println(d6.getYear()); System.out.println(d6.getMonth()); System.out.println(d6.getDayOfYear()); System.out.println(d6.getDayOfMonth()); System.out.println(d6.getDayOfWeek()); System.out.println(d6.getHour()); System.out.println(d6.getMinute()); System.out.println(d6.getSecond()); System.out.println(d6.getNano()); // 时间增减 LocalDateTime d7 = d6.minusDays(1); LocalDateTime d8 = d7.plus(1, IsoFields.QUARTER_YEARS); // LocalDate 即年月日 无时分秒 // LocalTime 即时分秒 无年月日 // API 和 LocalDateTime 类似就不演示了 &#125; public static void testZonedDateTime() &#123; // 即带有时区的 date-time 存储纳秒、时区和时差(避免与本地 date-time 歧义)。 // API 和 LocalDateTime 类似, 只是多了时差(如 2013-12-20T10:35:50.711+08:00[Asia/Shanghai]) ZonedDateTime now = ZonedDateTime.now(); System.out.println(now); ZonedDateTime now2 = ZonedDateTime.now(ZoneId.of("Europe/Paris")); System.out.println(now2); // 其他的用法也是类似的 就不介绍了 ZonedDateTime z1 = ZonedDateTime.parse("2013-12-31T23:59:59Z[Europe/Paris]"); System.out.println(z1); &#125; public static void testDuration() &#123; // 表示两个瞬时时间的时间段 Duration d1 = Duration.between(Instant.ofEpochMilli(System.currentTimeMillis() - 12323123), Instant.now()); // 得到相应的时差 System.out.println(d1.toDays()); System.out.println(d1.toHours()); System.out.println(d1.toMinutes()); System.out.println(d1.toMillis()); System.out.println(d1.toNanos()); // 1 天时差 类似的还有如 ofHours() Duration d2 = Duration.ofDays(1); System.out.println(d2.toDays()); &#125; public static void testChronology() &#123; // 提供对 java.util.Calendar 的替换, 提供对年历系统的支持 Chronology c = HijrahChronology.INSTANCE; ChronoLocalDateTime d = c.localDateTime(LocalDateTime.now()); System.out.println(d); &#125; /** * 新旧日期转换 */ public static void testNewOldDateConversion()&#123; Instant instant=new Date().toInstant(); Date date=Date.from(instant); System.out.println(instant); System.out.println(date); &#125; public static void main(String[] args) throws InterruptedException &#123; testClock(); testInstant(); testLocalDateTime(); testZonedDateTime(); testDuration(); testChronology(); testNewOldDateConversion(); &#125;&#125; 12345678910// 取本月第 1 天:LocalDate firstDayOfThisMonth = today.with(TemporalAdjusters.firstDayOfMonth()); // 2014-12-01// 取本月第 2 天:LocalDate secondDayOfThisMonth = today.withDayOfMonth(2); // 2014-12-02// 取本月最后一天, 再也不用计算是 28, 29, 30 还是 31:LocalDate lastDayOfThisMonth = today.with(TemporalAdjusters.lastDayOfMonth()); // 2014-12-31// 取下一天:LocalDate firstDayOf2015 = lastDayOfThisMonth.plusDays(1); // 变成了 2015-01-01// 取 2015 年 1 月第一个周一, 这个计算用 Calendar 要死掉很多脑细胞:LocalDate firstMondayOf2015 = LocalDate.parse("2015-01-01").with(TemporalAdjusters.firstInMonth(DayOfWeek.MONDAY)); // 2015-01-05 与 Joda-Time 的区别其实 JSR310 的规范领导者 Stephen Colebourne, 同时也是 Joda-Time 的创建者, JSR310 是在 Joda-Time 的基础上建立的, 参考了绝大部分的 API, 但并不是说 JSR310=JODA-Time, 下面几个比较明显的区别是 最明显的变化就是包名(从 org.joda.time 以及 java.time) JSR310 不接受 NULL 值, Joda-Time 视 NULL 值为 0 JSR310 的计算机相关的时间 (Instant) 和与人类相关的时间 (DateTime) 之间的差别变得更明显 JSR310 所有抛出的异常都是 DateTimeException 的子类。虽然 DateTimeException 是一个 RuntimeException 总结对比旧的日期 API Java.time java.util.Calendar 以及 Date 流畅的 API 不流畅的 API 实例不可变 实例可变 线程安全 非线程安全 日期与时间处理 API, 在各种语言中, 可能都只是个不起眼的 API, 如果你没有较复杂的时间处理需求, 可能只是利用日期与时间处理 API 取得系统时间, 简单做些显示罢了, 然而如果认真看待日期与时间, 其复杂程度可能会远超过你的想象, 天文、地理、历史、政治、文化等因素, 都会影响到你对时间的处理。所以在处理时间上, 最好选用 JSR310(如果你用 java8 的话就实现 310 了), 或者 Joda-Time。 不止是 java 面临时间处理的尴尬, 其他语言同样也遇到过类似的问题, 比如 Arrow: Python 中更好的日期与时间处理库 Moment.js: JavaScript 中的日期库 Noda-Time: .NET 阵营的 Joda-Time 的复制]]></content>
      <categories>
        <category>Java</category>
        <category>日期和时间</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>日期和时间</tag>
        <tag>Java 8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些常用的修复 Windows 系统的命令行及使用教程]]></title>
    <url>%2F2017-03%2FSystem-Repair-Command-Of-Windows.html</url>
    <content type="text"><![CDATA[1 基础对于一些问题, 我们可以先试一试 sfc 命令. 简介: SFC 命令用于检查系统文件, 并可以在重启 win7 后恢复受损的系统文件. 参数 解释 /scannow 立即扫描所有受保护的系统文件. /scanonce 一次扫描所有受保护的系统文件. /scanboot 每次重启时扫描所有受保护的系统文件. /revert 将扫描返回到默认操作. /purgecache 立即清除 “Windows 文件保护” 文件缓存, 并扫描所有受保护的系统文件. /cachesize=x 设置 “Windows 文件保护” 文件缓存的大小, 以 MB 为单位. 例如: 在 “命令提示符” 窗口中, 输入命令 1SFC /Scannow 此命令在完成扫描及重启操作后, 一些基本的文件损坏故障就可以恢复. 2 进阶篇如果上面的方法还无法修复问题, 那么就可以试一试 dism 命令了. DISM 命令功能强大复杂, 这里就不细说. 操作: 管理员运行 CMD 后, 输入 1Dism/Online /Cleanup-Image /A 其中, A 部分有三种格式 Scanhealth 扫描所有系统文件的完整性. Checkhealth 检查系统文件, 并与服务器标准对比, 然后报告偏差文件 Restorehealth 将所有有偏差的文件恢复至于服务器样本一致 注意: 运行上面的命令时, 需要电脑有可用网络. 并且, 这条命令比较花时间, 比较消耗电脑资源, 所以修复期间建议可以干点别的. 补充: 对于没有网的小伙伴, 有没有不要网络的方法呢？答案当然是有的. 前提是你要有一个当前系统的 ISO 安装镜像 (其实只需要用到里面的 install.wim 文件). 步骤: 如果是 ISO 镜像, 先挂载当前系统镜像. 如果是 install.wim 请略过这一步 使用 wimtool 工具挂载 install.wim 文件到指定的目录, 这里以 d:\mount 为例. 小伙伴们自己修复时只需要把 D:\mount 改为自己的路径就可以了. 挂载完成后管理员身份运行下面的命令 Dism /Online/Cleanup-Image /RestoreHealth /Source:D:\mount\windows /LimitAccess 不喜欢命令的童鞋推荐使用初雨论坛的 DISM++ 工具. 参考https://www.landiannews.com/archives/15698.html]]></content>
      <categories>
        <category>Windows</category>
        <category>cmd</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>cmd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UEFI & GPT]]></title>
    <url>%2F2017-03%2FUEFI-And-GPT.html</url>
    <content type="text"><![CDATA[1 引导基础篇: 什么是 UEFI, 什么是 GPT本文目的是想用最简单直白的语言把内容写出来, 让每个人都能轻松读懂。当然, 如果你已经对这些内容有了很深的理解的话, 这篇文章除了浪费你的时间外恐怕是别无益处了, 建议去 UEFI 官方网站下载 UEFI 规范文档, 那里详细阐述了 UEFI、GPT 的每个技术细节。 其实 UEFI、GPT 都不是什么新鲜玩意了, 只不过因为预装 Win8 电脑指定采用了这一标准所以仿佛是在一夜之间关于 UEFI 启动 Windows 的文章就遍地开花了, 很多人大概也是头一次听说世上还有这玩意。既然 UEFI、GPT 是 “新” 技术, 那么就必然有较旧技术 (BIOS+MBR) 更先进、更 NB 的地方。到底 NB 在哪里呢？下面来简单比较一下: GPT 及其优势GPT 和 MBR 是两种不同的分区方案。目前在 Windows 下广泛采用的磁盘分区方案仍然是 MBR 分区结构, 但不容怀疑 GPT 是今后的趋势。我们可将 MBR 磁盘分区结构用下图简单表示(Windows 下基本磁盘、4 个主分区): MBR 分区结构 为了方便计算机访问硬盘, 把硬盘上的空间划分成许许多多的区块(英文叫 sectors, 即扇区), 然后给每个区块分配一个地址, 称为逻辑块地址(即 LBA)。 在 MBR 磁盘的第一个扇区内保存着启动代码和硬盘分区表。启动代码的作用是指引计算机从活动分区引导启动操作系统 (BIOS 下启动操作系统的方式); 分区表的作用是记录硬盘的分区信息。在 MBR 中, 分区表的大小是固定的, 一共可容纳 4 个主分区信息。在 MBR 分区表中逻辑块地址采用 32 位二进制数表示, 因此一共可表示 2^32(2 的 32 次方) 个逻辑块地址。如果一个扇区大小为 512 字节, 那么硬盘最大分区容量仅为 2TB。 GPT 磁盘分区结构可用下图简单表示(Windows 下基本磁盘): GPT 分区结构 可以看到, 在 GTP 磁盘的第一个数据块中同样有一个与 MBR(主引导记录)类似的标记, 叫做 PMBR。PMBR 的作用是, 当使用不支持 GPT 的分区工具时, 整个硬盘将显示为一个受保护的分区, 以防止分区表及硬盘数据遭到破坏。UEFI 并不从 PMBR 中获取 GPT 磁盘的分区信息, 它有自己的分区表, 即 GPT 分区表。 GPT 的分区方案之所以比 MBR 更先进, 是因为在 GPT 分区表头中可自定义分区数量的最大值, 也就是说 GPT 分区表的大小不是固定的。在 Windows 中, 微软设定 GPT 磁盘最大分区数量为 128 个。另外, GPT 分区方案中逻辑块地址 (LBA) 采用 64 位二进制数表示, 可以计算一下 2^64 是一个多么庞大的数据, 以我们的需求来讲完全有理由认为这个大小约等于无限。除此之外, GPT 分区方案在硬盘的末端还有一个备份分区表, 保证了分区信息不容易丢失。 Windows 操作系统对 GPT 磁盘的支持因为 BIOS 无法识别 GPT 分区, 所以 BIOS 下 GPT 磁盘不能用于启动操作系统, 在操作系统提供支持的情况下可用于数据存储。 UEFI 可同时识别 MBR 分区和 GPT 分区, 因此 UEFI 下, MBR 磁盘和 GPT 磁盘都可用于启动操作系统和数据存储。不过微软限制, UEFI 下使用 Windows 安装程序安装操作系统是只能将系统安装在 GPT 磁盘中。 下表列出了 Windows 各版本操作系统对 GPT 磁盘的支持程度: 32 位 Windows 对 GPT 分区支持情况 64 位 Windows 对 GPT 分区支持情况 UEFI 及其优势UEFI 是 BIOS 的一种升级替代方案。关于 BIOS 和 UEFI 二者的比较, 网络上已经有很多相关的文章, 这里不再赘述, 仅从系统启动原理方面来做比较。UEFI 之所以比 BIOS 强大, 是因为 UEFI 本身已经相当于一个微型操作系统, 其带来的便利之处在于: 首先, UEFI 已具备文件系统的支持, 它能够直接读取 FAT 分区中的文件; 什么是文件系统？简单说, 文件系统是操作系统组织管理文件的一种方法, 直白点说就是把硬盘上的数据以文件的形式呈现给用户。Fat32、NTFS 都是常见的文件系统类型。 其次, 可开发出直接在 UEFI 下运行的应用程序, 这类程序文件通常以 efi 结尾。 既然 UEFI 可以直接识别 FAT 分区中的文件, 又有可直接在其中运行的应用程序。那么完全可以 将 Windows 安装程序做成 efi 类型应用程序, 然后把它放到任意 fat 分区中直接运行即可, 如此一来安装 Windows 操作系统这件过去看上去稍微有点复杂的事情突然就变非常简单了, 就像在 Windows 下打开 QQ 一样简单。而事实上, 也就是这么一回事。 要知道, 这些都是 BIOS 做不到的。因为 BIOS 下启动操作系统之前, 必须从硬盘上指定扇区读取系统启动代码 (包含在主引导记录中), 然后从活动分区中引导启动操作系统。对扇区的操作远比不上对分区中文件的操作更直观更简单, 所以在 BIOS 下引导安装 Windows 操作系统, 我们不得不使用一些工具对设备进行配置以达到启动要求。而在 UEFI 下, 这些统统都不需要, 不再需要主引导记录, 不再需要活动分区, 不需要任何工具, 只要复制安装文件到一个 FAT32(主) 分区 / U 盘中, 然后从这个分区 / U 盘启动, 安装 Windows 就是这么简单。后面会有专门的文章来详细介绍 UEFI 下安装 Windows7、8 的方法。 2 引导实践篇(一): 切换到 UEFI 启动, 准备安装介质如果只单纯比较 UEFI 引导和 BIOS 引导, 那么毫无疑问 UEFI 引导要简单很多。不过现在的主板大都是同时兼容 BIOS 和 UEFI 引导方式, 所以在实际操作前还需要确认一些东西。详见下文。 1、我的电脑支不支持 UEFI 启动？要通过 UEFI 方式启动 Windows7/8, 电脑需要支持 UEFI 启动。预装 Win8 的电脑都支持 UEFI 启动。近两年的主板很多也都支持 UEFI 启动。开机出现品牌标识按 F2(或者 DEL、ESC 等按键, 不通品牌按键不通)进入 BIOS/UEFI 设置, 在 Boot 项或类似项中查看有无 EFI/UEFI 相关选项, 如果有, 那么说明电脑支持 UEFI 启动。下图是联想 Y480UEFI 选项: 联想 Y480 UEFI 引导选项 2、想要 UEFI 启动, 我该选择安装什么操作系统？目前几乎所有支持 UEFI 的个人电脑都是 64 位 UEFI 系统, 仅支持 64 位 Vista、64 位 Win7 以及 64 位 Win8/8.1。所以你只能从这几个操作系统中选择。切记, 只有原版镜像支持光盘 UEFI 启动, 一定要选择下载微软原版系统！建议选择的版本: 不管是 Win7、Win8 或 Win8.1, 都建议选择专业版; 激活都选择KMS 激活方式。 提示: 已经下载了 Win7 旗舰版镜像的朋友, 如采用 U 盘安装或从硬盘引导安装(后面会讲到), 在准备好安装介质后只需要删除 sources 文件夹下面的 ei.cfg 文件即可选择安装除企业版之外任意版本, 包括专业版。 3、预装 Win8 的电脑想要安装 64 位 Win7、Vista 需关闭安全启动, 并开启 CSM具体选项:① 某些预装 Win8 的电脑有名为 OS Optimized Defaults 的设置选项, 需要将这个选项设置为 Disabled。② 找到类似 “Boot mode” 或者 “UEFI/Legacy Boot” 设置项, 将其设置为 “UEFI” 或类似选项。③ 如果能找到 “Lunch CSM” 或类似选项, 将其设为 “Enabled”, 即开启状态。④ 找到 “Secure Boot” 或类似选项, 将其设为 “Disabled”, 即关闭状态。不关闭安全启动, 无法安装 Win8 之外的操作系统。详细教程: 《预装 64 位 Win8/8.1 电脑安装 64 位 Win7 详细过程(单 / 双系统)》 4、(非安装双系统跳过此步)为将要安装的操作系统开辟新的分区如果想要在现有系统基础上再安装一个操作系统组成双系统, 首先你现有的系统必须为 UEFI+GPT 引导(例如, 预装 Win8 系统), 否则只能在安装过程中删除硬盘所有数据自动完成转换。确定是否为 GPT 分区: 打开磁盘管理, 找到硬盘 0, 然后右键点击。如果有灰色选项 “转换成 MBR 磁盘” 则说明硬盘为 GPT 分区, 如下图①所示, 并且系统一定为 UEFI 引导。开辟新分区的方法: 打开磁盘管理, 在剩余空间较大的分区上右键点击, 选择 “压缩”, 如下图 2 所示。然后输入合适的大小压缩即可(推荐不小于 50G)。 查看硬盘分区结构 &amp; 压缩分区空间 5、准备安装介质有三种选择: 一、可以将微软原版 64 为 Win7 镜像、64 位 Win8 镜像直接刻入光盘, 从光盘启动安装。二、可以准备一个容量不小于 4G 的 U 盘, 格式化为 FAT32, 然后复制 64 为 Win7 镜像或 64 位 Win8 镜像中的所有文件到 U 盘根目录, 文件结构如下图①所示。如果安装的是 64 位 Win7, 还需要下载 bootx64.efi 文件(点击下载), 然后在 U 盘中 efi 文件夹下新建名为 boot 的文件夹, 把 bootx64.efi 文件放到里边。如下图②所示。 启动介质中文件结构 三、也可以在电脑硬盘或者移动硬盘 (都必须为 GPT 分区) 任意位置压缩出一个分区(大小在 4G~30G 之间), 格式化为 FAT32, 然后参考上一条复制文件即可。 特别提示: 如果下载的是 Win7 旗舰版, 想要安装专业版, 只需在准备好安装介质后删掉 sources 目录中的 ei.cfg 文件即可选择安装除企业版之外的任意版本, 包括专业版。 OK, 要准备的就这些。安装过程请参考《UEFI+GPT 引导实践篇(二): UEFI 引导安装 64 位 Win7/Win8》。 3 UEFI+GPT 引导实践篇(二): UEFI 引导安装 64 位 Win7/Win8下文是在联想 Y480 笔记本上以 UEFI 方式启动安装 Windows8 的全过程, 安装 Windows7 过程基本相同。注意, 如果你的电脑硬盘是 MBR 分区结构, 安装过程中将要删除硬盘上所有数据, 请安装前备份硬盘上的所有个人文件。 准备容量不小于 4G 的 U 盘一个, 格式化为 FAT32。然后复制 64 位 Win8 镜像中的所有文件到 U 盘根目录。 注: 如果安装的是 64 位 Win7, 还需要下载 bootx64.efi 文件(点击下载), 然后在 U 盘中 efi 文件夹下新建名为 boot 的文件夹, 把 bootx64.efi 文件放到里边。预装 Win8/8.1 电脑安装 Win7 还需要更改几个 BIOS 设置, 相关准备工作请参考《UEFI+GPT 引导准备篇: 切换到 UEFI 启动, 准备安装介质》或参考专门针对此情况的完整教程《预装 64 位 Win8/8.1 电脑安装 64 位 Win7 详细过程(单 / 双系统)》。 重启电脑, 出现联想标识时按 F2 打开 BIOS 设置界面, 切换到 “boot” 设置界面将 UEFI Boot 设置项设置为 “Enable”, 这样就开启了 UEFI。按 F10 键, 保存设置。 重启, 在出现联想标识时按 F12 键, 此时出现启动设备选择界面, 我们可以看到这里有两个 USB 设备。图中被选中的名称前面可以看到 “EFI” 字样, 这就表示该设备是可以 EFI 引导的设备。我们就需要选择这一项。 选中图中的选项后, 电脑会开始加载 Windows 预安装环境文件。(注: 小编实际操作时安装的是 64 位 Win7, 但是手机拍下来的图很不清晰, 所以就在虚拟机中在 UEFI 模式下安装了 64 位 Win8, 二者无区别) 稍等出现下面的界面, 默认即可。 上图: 点下一步 上图: 点 “现在安装” 上图: 这里选择 “自定义”, 进入分区选择界面。如果你的电脑硬盘已经为 GPT 分区结构, 那么直接选择目标分区, 将其格式化, 然后选中点下一步即可。下面是硬盘位 MBR 分区结构的情况: 上图: 如果你的电脑硬盘为 MBR 分区结构, 那么会看到 “无法在驱动器 X 的分区 x 上安装 Windows” 的提示, 这时候需要点击 “驱动器选项(高级)”, 然后删除硬盘上所有的分区使得整个硬盘变成一块未分配空间。提示: 在这一步你也可以按 Shift+F10 或者 Shift+Fn+F10 调出命令提示符窗口, 然后借助 diskpart 工具将硬盘转换为 GPT 分区。命令详见《Diskpart 工具应用两则: MBR/GPT 分区转换 &amp; 基本 / 动态磁盘转换》。 上图: 整个硬盘已成为一块未分配空间。点击 “新建”, 输入想要为系统安装分区分配的空间大小(推荐不小于 50G), 点击 “应用”。 上图: 此时提示将创建额外的分区, 点击 “确定”。之后安装程序会自动将硬盘转换为 GPT 分区。创建完成后你将看到四个创建好的分区, 如下图所示。关于这些分区的作用详见《用于引导 Windows 的 GPT 磁盘 (预装 Win8 电脑) 各分区作用详解》 选择第一个主分区, 然后点 “下一步” 就开始安装了。之后根据提示操作即可完成系统安装。 后记实际上只有遵循 UEFI 规范的主板才能按照 efi/boot/bootx64.efi 路径启动 Windows 安装程序, 大多数主板都是遵循这一规范的。如果不遵循这一规范, 那么你需要手动选择从该文件启动, 或者需要在 UEFI Shell 中以命令的方式启动 bootx64.efi。关于这些后面会有文章来详解。另外关于预装 Win8 电脑改装 Win7 或者安装 Win8+Win7 双系统都会有专门教程。 参考整理自:http://www.iruanmi.com/what-is-gpt-and-what-is-uefi/http://www.iruanmi.com/get-ready-for-installing/http://www.iruanmi.com/install-win7-or-win8-on-uefi-platform/]]></content>
      <categories>
        <category>计算机相关</category>
      </categories>
      <tags>
        <tag>装机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何为 RHEL / CentOS 7.x / 6.x / 5.x 启用 EPEL 存储库]]></title>
    <url>%2F2017-01%2FEnable-Epel-Repo-For-RHEL-And-CentOS.html</url>
    <content type="text"><![CDATA[待整理 2016 年 8 月 17 日CentOS 系统 RedHat 这个指南手册显示了您将如何启用 RHEL / CentOS 的 6/5 EPEL 软件库, 以便使用 yum 命令来安装额外的标准开源软件包. 另请参阅 : 安装和 RHEL 启用 RPMForge 软件库 / CentOS 7/6/5/4 什么是 EPELEPEL(额外的企业版 Linux 软件包) 是开源和基于免费的社区信息库项目由 Fedora 的团队, 提供 100％的高品质的附加软件包的 Linux 发行版, 包括 RHEL(红帽企业 Linux), CentOS 的, 与科学 Linux 操作系统. Epel 项目不是 RHEL / Cent OS 的一部分, 但它是为主要的 Linux 发行版本提供的, 它提供了许多开源软件包, 如网络, 系统管理, 编程, 监视等等. 大多数 epel 软件包由 Fedora 软件库维护. 为什么我们使用 EPEL 存储库 提供许多开源包以通过 Yum 安装. EPEL 仓库是 100％开源和免费使用. 它不提供任何核心重复的软件包和没有兼容性问题. 所有 epel 包都由 Fedora 库维护. 如何在 RHEL / CentOS 7/6/5 中启用 EPEL 存储库首先, 您需要使用 wget 下载的文件, 然后使用你的系统上 RPM 启用 EPEL 软件库安装它. 根据您的 Linux 操作系统版本使用以下链接. 请确保您必须是 root 用户 ). RHEL / CentOS 7 64 位123## RHEL/CentOS 7 64-Bit ### wget http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-8.noarch.rpm# rpm -ivh epel-release-7-8.noarch.rpm RHEL / CentOS 6 32-64 位123456## RHEL/CentOS 6 32-Bit ### wget http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm# rpm -ivh epel-release-6-8.noarch.rpm## RHEL/CentOS 6 64-Bit ### wget http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm# rpm -ivh epel-release-6-8.noarch.rpm RHEL / CentOS 5 32-64 位123456## RHEL/CentOS 5 32-Bit ### wget http://download.fedoraproject.org/pub/epel/5/i386/epel-release-5-4.noarch.rpm# rpm -ivh epel-release-5-4.noarch.rpm## RHEL/CentOS 5 64-Bit ### wget http://download.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm# rpm -ivh epel-release-5-4.noarch.rpm RHEL / CentOS 4 32-64 位123456## RHEL/CentOS 4 32-Bit ### wget http://download.fedoraproject.org/pub/epel/4/i386/epel-release-4-10.noarch.rpm# rpm -ivh epel-release-4-10.noarch.rpm## RHEL/CentOS 4 64-Bit ### wget http://download.fedoraproject.org/pub/epel/4/x86_64/epel-release-4-10.noarch.rpm# rpm -ivh epel-release-4-10.noarch.rpm 如何验证 EPEL 仓库您需要运行以下命令来验证是否已启用 EPEL 存储库. 一旦你运行命令, 你将看到 epel 存储库. 1# yum repolist 示例输出 12345678910111213141516Loaded plugins: downloadonly, fastestmirror, prioritiesLoading mirror speeds from cached hostfile* base: centos.aol.in* epel: ftp.cuhk.edu.hk* extras: centos.aol.in* rpmforge: be.mirror.eurid.eu* updates: centos.aol.inReducing CentOS-5 Testing to included packages onlyFinished1469 packages excluded due to repository priority protectionsrepo id repo name statusbase CentOS-5 - Base 2,718+7epel Extra Packages for Enterprise Linux 5 - i386 4,320+1,408extras CentOS-5 - Extras 229+53rpmforge Red Hat Enterprise 5 - RPMforge.net - dag 11,251repolist: 19,075 如何使用 EPEL 仓库您需要使用 yum 命令用于搜索和安装软件包. 例如, 我们使用 EPEL 仓库搜索的 zabbix 软件包, 让我们看看它是否可用下 EPEL. 1# yum --enablerepo=epel info zabbix 示例输出 1234567891011Available PackagesName : zabbixArch : i386Version : 1.4.7Release : 1.el5Size : 1.7 MRepo : epelSummary : Open-source monitoring solution for your IT infrastructureURL : http://www.zabbix.com/License : GPLDescription: ZABBIX is software that monitors numerous parameters of a network. 让我们使用 EPEL 仓库选项 -enablerepo = EPEL 开关安装的 zabbix 软件包. 1# yum --enablerepo=epel install zabbix 注: EPEL 配置文件位于 /etc/yum.repos.d/epel.repo. 这样, 您就可以安装多达使用 EPEL 回购高标准的开源软件包.]]></content>
      <categories>
        <category>Linux</category>
        <category>运维</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用 shell 实现 tree 命令]]></title>
    <url>%2F2017-01%2FImplement-Tree-Command-Use-Shell.html</url>
    <content type="text"><![CDATA[待整理 http://yijiebuyi.com/blog/c0defa3a47d16e675d58195adc35514b.html]]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10/Ubuntu 双系统]]></title>
    <url>%2F2016-12%2FDual-Ssystem-Of-Windows-And-Ubuntu.html</url>
    <content type="text"><![CDATA[前置EasyBCD 软件 + 镜像文件 Step 1 划分空间在 Win10 磁盘管理分出大约 20 G 的空间(可以通过 压缩卷 获得), 划好要给 Ubuntu 的分区后, 删除卷, 划分出来的部分显示为黑色. Step 2 禁用安全启动禁用主板 (BIOS or UEFI) 上的 Secure Boot: 设置 - 更新和安全 - 恢复 - 高级启动 - 疑难解答 - 高级选项 - UEFI 固件设置 - 重新启动 会进入类似 BIOS 的界面: Boot - Secure Boot - Disable Step 3 安装将 Ubuntu 刻录到 U 盘上, 重启(设置 - 更新和安全 - 恢复 - 高级启动), 从 U 盘启动, 进入 Ubuntu 安装界面. 我们要手动分盘符, 所以, 出现下图时, 选择 “其它选项” 选中其中标有 “空闲” 的盘符, 这个盘符就是我们用于安装 Ubuntu 的 20 G 空间, 别动别的盘符, 点击下方的 “+” 接下来, 我们要进行四次分区, 每次都是从 “空闲” 中分出部分区域 第一次分区 选中 “空闲”, 点 “+”, 如下设置 大小: 200 MB 新分区的类型: 逻辑分区 新分区的位置: 空间起始位置 用于: EXT4 日志文件系统 挂载点: /boot (双系统引导时需要) 如下图所示 第二次分区 选中 “空闲”, 点 “+”, 如下设置 大小: 2048 MB 新分区的类型: 逻辑分区 新分区的位置: 空间起始位置 用于: 交换空间 挂载点: (不设置) 如下图所示 第三次分区 选中 “空闲”, 点 “+”, 如下设置 大小: 10240 MB 新分区的类型: 主分区 新分区的位置: 空间起始位置 用于: EXT4 日志文件系统 挂载点: “/“ 如下图所示 第四次分区 选中 “空闲”, 点 “+”, 如下设置 大小: (剩余全部空间, 剩下显示多少, 就多少) 新分区的类型: 主分区 新分区的位置: 空间起始位置 用于: EXT4 日志文件系统 挂载点: /home 如下图所示 分区设置完毕后, 下方还有一项 “安装启动引导器的设备”, 默认就如如下图所示那样 sda, 如果选择默认, 则是用 Ubuntu 的 grub 引导 Windows 和 Ubuntu, 卸载 Ubuntu 时可能会麻烦些. 如果想用 Windows 的引导来引导 Windows 和 Ubuntu(推荐), 请选择 /boot 所在的盘符, 然后在 Windows 下安装引导类的软件(如: EasyBCD, 见 Step 4), 才能启动 Ubuntu. 优点是不管怎么搞, 不会把 Windows 搞坏. 用 Windows 引导 Ubuntu 最大的好处就是, 当不再需要 Ubuntu 的时候, 直接在 Windows 磁盘管理中将其所有分区删除, 然后将 EasyBCD 中对应条目删除即可. PS1: 然后点击 “现在安装”, 如果出错不能继续, 说什么分区不对, 就把是主分区的新分区, 都重新设置为逻辑分区, 就可以了. PS2: 设置好之后记住 /boot 所在的分区位置, 下面要用到. Step 4进入 Win10 之后, 安装 EasyBCD, 然后新增一个启动项, 选择 Linux/Grub2, 启动位置选择刚才 /boot 所在的位置(如果没有给 /boot 分区, 则是 / 所在的位置), 保存退出, 重启就可以看到 Ubuntu 的入口了. 如下图所示]]></content>
      <categories>
        <category>生产力</category>
      </categories>
      <tags>
        <tag>生产力</tag>
        <tag>双系统</tag>
        <tag>装机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh 非交互式远程执行命令]]></title>
    <url>%2F2016-11%2FSSH-Remote-Command-Execution-Non-Interactive.html</url>
    <content type="text"><![CDATA[待整理 非交互式在远程主机上执行命令或者脚本可以帮助我们快速完成一些任务. 比如, 在集群环境中, 同时在各个结点上的日志文件中查询特定的关键字 ssh 命令格式如下: 12345678ssh [-1246AaCfgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec] [-D [bind_address:]port] [-e escape_char] [-F configfile] [-I pkcs11] [-i identity_file] [-L [bind_address:]port:host:hostport] [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port] [-R [bind_address:]port:host:hostport] [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]] [user@]hostname [command] 主要参数说明:-l 指定登入用户-p 设置端口号-f 后台运行, 并推荐加上 -n 参数-n 将标准输入重定向到 /dev/null, 防止读取标准输入-N 不执行远程命令, 只做端口转发-q 安静模式, 忽略一切对话和错误提示-T 禁用伪终端配置 ssh 执行远程命令格式: ssh [options][remote host][command] 假设远程服务器 IP 是 192.168.110.34 例: 查看远程服务器的 cpu 信息 123456789101112131415161718192021222324252627282930313233343536373839www-online@onlinedev01:~$ ssh -l www-online 192.168.110.34 "cat /proc/cpuinfo"www-online@192.168.110.34's password:processor : 0vendor_id : GenuineIntelcpu family : 6model : 26model name : Intel(R) Xeon(R) CPU E5506 @ 2.13GHzstepping : 5cpu MHz : 2128.000cache size : 4096 KBfpu : yesfpu_exception : yescpuid level : 11wp : yesflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology tsc_reliable nonstop_tsc aperfmperf pni ssse3 cx16 sse4_1 sse4_2 popcnt hypervisor lahf_lmbogomips : 4256.00clflush size : 64cache_alignment : 64address sizes : 40 bits physical, 48 bits virtualpower management:processor : 1vendor_id : GenuineIntelcpu family : 6model : 26model name : Intel(R) Xeon(R) CPU E5506 @ 2.13GHzstepping : 5cpu MHz : 2128.000cache size : 4096 KBfpu : yesfpu_exception : yescpuid level : 11wp : yesflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology tsc_reliable nonstop_tsc aperfmperf pni ssse3 cx16 sse4_1 sse4_2 popcnt hypervisor lahf_lmbogomips : 4260.80clflush size : 64cache_alignment : 64address sizes : 40 bits physical, 48 bits virtualpower management: 例: 执行远程服务器的 sh 文件首先在远程服务器的 /home/www-online/ 下创建一个 uptimelog.sh 脚本 12345#!/bin/bashuptime &gt;&gt; 'uptime.log'exit 0 使用 chmod 增加可执行权限 1chmod u+x uptimelog.sh 在本地调用远程的 uptimelog.sh 1ssh -l www-online 192.168.110.34 "/home/www-online/uptimelog.sh" 执行完成后, 在远程服务器的 /home/www-online/ 中会看到 uptime.log 文件, 显示 uptime 内容 12www-online@nmgwww34:~$ tail -f uptime.log21:07:34 up 288 days, 8:07, 1 user, load average: 0.05, 0.19, 0.31 例: 执行远程后台运行 sh首先把 uptimelog.sh 修改一下, 修改成循环执行的命令。作用是每一秒把 uptime 写入 uptime.log 123456789#!/bin/bashwhile :do uptime &gt;&gt; 'uptime.log' sleep 1doneexit 0 我们需要这个 sh 在远程服务器以后台方式运行, 命令如下: 12www-online@onlinedev01:~$ ssh -l www-online 192.168.110.34 "/home/www-online/uptimelog.sh &amp;"www-online@192.168.110.34's password: 输入密码后, 发现一直停住了, 而在远程服务器可以看到, 程序已经以后台方式运行了。 12www-online@nmgwww34:~$ ps aux|grep uptimelog.sh1007 20791 0.0 0.0 10720 1432 ? S 21:25 0:00 /bin/bash /home/www-online/uptimelog.sh 原因是因为 uptimelog.sh 一直在运行, 并没有任何返回, 因此调用方一直处于等待状态。我们先 kill 掉远程服务器的 uptimelog.sh 进程, 然后对应此问题进行解决。 ssh 调用远程命令后不能自动退出解决方法可以将标准输出与标准错误输出重定向到 /dev/null, 这样就不会一直处于等待状态。 123www-online@onlinedev01:~$ ssh -l www-online 192.168.110.34 "/home/www-online/uptimelog.sh &gt; /dev/null 2&gt;&amp;1 &amp;"www-online@192.168.110.34's password:www-online@onlinedev01:~$ 但这个 ssh 进程会一直运行在后台, 浪费资源, 因此我们需要自动清理这些进程。 实际上, 想 ssh 退出, 我们可以在 ssh 执行完成后 kill 掉 ssh 这个进程来实现。首先, 创建一个 sh 执行 ssh 的命令, 这里需要用到 ssh 的 -f 与 -n 参数, 因为我们需要 ssh 也以后台方式运行, 这样才可以获取到进程号进行 kill 操作。创建 ssh_uptimelog.sh, 脚本如下 1234567891011#!/bin/bashssh -f -n -l www-online 192.168.110.34 "/home/www-online/uptimelog.sh &amp;" # 后台运行 sshpid=$(ps aux | grep "ssh -f -n -l www-online 192.168.110.34 /home/www-online/uptimelog.sh" | awk '&#123;print $2&#125;' | sort -n | head -n 1) # 获取进程号echo "ssh command is running, pid:$&#123;pid&#125;"sleep 3 &amp;&amp; kill $&#123;pid&#125; &amp;&amp; echo "ssh command is complete" # 延迟 3 秒后执行 kill 命令, 关闭 ssh 进程, 延迟时间可以根据调用的命令不同调整exit 0 可以看到, 3 秒后会自动退出 12345www-online@onlinedev01:~$ ./ssh_uptimelog.shwww-online@192.168.110.34's password:ssh command is running, pid:10141ssh command is completewww-online@onlinedev01:~$ 然后查看远程服务器, 可以见到 uptimelog.sh 在后台正常执行。 12www-online@nmgwww34:~$ ps aux|grep uptime1007 28061 0.1 0.0 10720 1432 ? S 22:05 0:00 /bin/bash /home/www-online/uptimelog.sh 查看 uptime.log, 每秒都有 uptime 数据写入。 123456www-online@nmgwww34:~$ tail -f uptime.log22:05:44 up 288 days, 9:05, 1 user, load average: 0.01, 0.03, 0.0822:05:45 up 288 days, 9:05, 1 user, load average: 0.01, 0.03, 0.0822:05:46 up 288 days, 9:05, 1 user, load average: 0.01, 0.03, 0.0822:05:47 up 288 days, 9:05, 1 user, load average: 0.01, 0.03, 0.0822:05:48 up 288 days, 9:05, 1 user, load average: 0.01, 0.03, 0.08 通过 SSH 命令远程执行命令首先需要建立相关主机间的信任关系 (无密码登录). 否则, 在执行命令前 SSH 命令会提示你输入远程主机的密码, 这就产生了系统与人的交互, 不利于脚本的自动化. 建立主机间信任关系的方法如下: 假设我们有两台主机. 主机名分别为 linuxa 和 linuxb. 首先在 linuxa 上以当前用户运行如下命令生成本主机的公钥和私钥文件: 1ssh-keygen -t rsa 上述命令执行后, 隐藏目录~/.ssh 下会出现两个文件: id_rsa 和 id_rsa.pub. 其中, id_rsa.pub 为公钥文件. 将该文件的内容追加到对端主机 linuxb 上~/.ssh 目录下的 authorized_keys 文件中. 若该文件不存在, 可自行创建之. 下面是一个 id_rsa.pub 文件示例的文件内容: 1ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAtbW/vKjrIkTfFjSJP9FyVb3kQStc31oBuiKVaCZzoejxSM2+ck6CB09l4BoFujpI0+omL4NptxkEAgkCGnMco2yXrVSOqhqyaQV2BnDPkyMoEq2MGB9hSc9xQKa+Q== viscent@viscent 接下来, 就可以在不输入密码的情况下在远程主机私执行命令了. 命令格式如下: ssh 远程用户名 @远程主机 IP 地址 ‘远程命令或者脚本’ 比如, 1ssh userA@192.168.0.6 'hostname' 上述命令执行后, 终端输出的是对端主机的主机名, 而不是你当前登录的主机的主机名. 说明 hostname 这个命令其实是在对端主机上运行的. 若要远程执行脚本, 只需要将上面的命令的第三个参数改为要执行的远程脚本的文件名全称即可. 比如: 1ssh userA@192.168.0.6 '/home/userA/script/test.sh' shell 远程执行: 经常需要远程到其他节点上执行一些 shell 命令, 如果分别 ssh 到每台主机上再去执行很麻烦, 因此能有个集中管理的方式就好了. 一下介绍两种 shell 命令远程执行的方法. 前提条件: 配置 ssh 免密码登陆 *. 对于简单的命令: 如果是简单执行几个命令, 则: 1ssh user@remoteNode "cd /home ; ls" 基本能完成常用的对于远程节点的管理了, 几个注意的点: 双引号, 必须有. 如果不加双引号, 第二个 ls 命令在本地执行 分号, 两个命令之间用分号隔开 *. 对于脚本的方式: 有些远程执行的命令内容较多, 单一命令无法完成, 考虑脚本方式实现: 1234567#!/bin/bashssh user@remoteNode &gt; /dev/null 2&gt;&amp;1 &lt;&lt; eeooffcd /hometouch abcdefg.txtexiteeooffecho done! 远程执行的内容在 “&lt;&lt; eeooff” 至 “eeooff” 之间, 在远程机器上的操作就位于其中, 注意的点: &lt;&lt; eeooff, ssh 后直到遇到 eeooff 这样的内容结束, eeooff 可以随便修改成其他形式.重定向目的在于不显示远程的输出了在结束前, 加 exit 退出远程节点 需要特别注意的是: 当远程脚本中使用了一些命令, 而这些命令被 Shell 解析器的识别依赖于 PATH 环境变量时, 该脚本需要在其第一行中包含执行 profile 文件的命令. 比如, 在 Bash 中, 该脚本的第一行为: 1source ~/.bashrc 否则, 远程脚本可能报一些命令无法找到的错误. 补充: 配置 hadoop 伪分布式的话, 需要本机对本机能够进行免密码访问, 直接将公钥文件 id_rsa.pub 的文件追加到 authorized_keys 中即可 12345sudo apt-get install sshcd ~ # 最好在要配置的用户的家目录下ssh-keygen -t rsa # 生成 rsa 密钥对, 也可以选 dsacp ./.ssh/id_rsa.pub ./.ssh/authorized_keys # id_rsa.pub 是公钥, id_rsa 是私钥ssh localhost # 验证, 第一次要输入'yes'确认加入 the list of known hosts 参考http://blog.csdn.net/fdipzone/article/details/23000201http://www.cnblogs.com/ilfmonday/p/ShellRemote.htmlhttp://viscent.iteye.com/blog/1706691]]></content>
      <categories>
        <category>Linux</category>
        <category>运维</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ps 配合 kill 使用]]></title>
    <url>%2F2016-10%2FCommand-Usage-ps-With-kill.html</url>
    <content type="text"><![CDATA[常规篇首先, 用 ps 查看进程, 方法如下: 12345678910$ ps -ef……will 1822 1 0 11:38 ? 00:00:49 gnome-terminalwill 1823 1822 0 11:38 ? 00:00:00 gnome-pty-helperwill 1824 1822 0 11:38 pts/0 00:00:02 bashwill 1827 1 4 11:38 ? 00:26:28 /usr/lib/firefox-3.6.18/firefox-binwill 1857 1822 0 11:38 pts/1 00:00:00 bashwill 1880 1619 0 11:38 ? 00:00:00 update-notifier……will 11946 1824 0 21:41 pts/0 00:00:00 ps -ef 或者: 12345678910$ ps -aux……will 1822 0.1 0.8 58484 18152 ? Sl 11:38 0:49 gnome-terminalwill 1823 0.0 0.0 1988 712 ? S 11:38 0:00 gnome-pty-helperwill 1824 0.0 0.1 6820 3776 pts/0 Ss 11:38 0:02 bashwill 1827 4.3 5.8 398196 119568 ? Sl 11:38 26:13 /usr/lib/firefox-3.6.18/firefox-binwill 1857 0.0 0.1 6688 3644 pts/1 Ss 11:38 0:00 bashwill 1880 0.0 0.6 41536 12620 ? S 11:38 0:00 update-notifier……will 11953 0.0 0.0 2716 1064 pts/0 R+ 21:42 0:00 ps -aux 此时如果我想杀了火狐的进程就在终端输入: 1kill -9 1827 其中 -9 指定了传递给进程的信号是 9, 即强制、尽快终止进程.1827 则是上面 ps 查到的火狐的 PID.简单吧, 但有个问题, 进程少了则无所谓, 进程多了, 就会觉得痛苦了, 无论是 ps -ef 还是 ps -aux, 每次都要在一大串进程信息里面查找到要杀的进程, 看的眼都花了. 进阶篇改进 1把 ps 的查询结果通过管道给 grep 查找包含特定字符串的进程. 管道符 “|” 用来隔开两个命令, 管道符左边命令的输出会作为管道符右边命令的输入. 123$ ps -ef | grep firefoxwill 1827 1 4 11:38 ? 00:27:33 /usr/lib/firefox-3.6.18/firefox-binwill 12029 1824 0 21:54 pts/0 00:00:00 grep --color=auto firefox 这次就清爽了. 然后就是 1kill -9 1827 还是嫌打字多? 改进 2 - 使用 pgrep一看到 pgrep 首先会想到什么? 没错, grep! pgrep 的 p 表明了这个命令是专门用于进程查询的 grep. 12$ pgrep firefox1827 看到了什么? 没错火狐的 PID, 接下来又要打字了: 1kill -9 1827 改进 3 - 使用 pidof看到 pidof 想到啥? 没错 pid of xx, 字面翻译过来就是 xx 的 PID. 12$ pidof firefox-bin1827 和 pgrep 相比稍显不足的是, pidof 必须给出进程的全名. 然后就是老生常谈: 1kill -9 1827 无论使用 ps 然后慢慢查找进程 PID 还是用 grep 查找包含相应字符串的进程, 亦或者用 pgrep 直接查找包含相应字符串的进程 pid, 然后手动输入给 kill 杀掉, 都稍显麻烦. 有没有更方便的方法? 有! 改进 41ps -ef | grep firefox | grep -v grep | cut -c 9-15 | xargs kill -9 说明:“grep firefox” 的输出结果是, 所有含有关键字 “firefox” 的进程.“grep -v grep” 是在列出的进程中去除含有关键字 “grep” 的进程.“cut -c 9-15” 是截取输入行的第 9 个字符到第 15 个字符, 而这正好是进程号 PID.“xargs kill -9” 中的 xargs 命令是用来把前面命令的输出结果 (PID) 作为 “kill -9” 命令的参数, 并执行该命令. “kill -9” 会强行杀掉指定进程.难道你不想抱怨点什么? 没错太长了 改进 5知道 pgrep 和 pidof 两个命令, 干嘛还要打那么长一串! 1pgrep firefox | xargs kill -9 改进 612$ ps -ef | grep firefox | awk '&#123;print $2&#125;' | xargs kill -9kill: No such process 有一个比较郁闷的地方, 进程已经正确找到并且终止了, 但是执行完却提示找不到进程.其中 awk ‘{print $2}’ 的作用就是打印 (print) 出第二列的内容. 根据常规篇, 可以知道 ps 输出的第二列正好是 PID. 就把进程相应的 PID 通过 xargs 传递给 kill 作参数, 杀掉对应的进程. 改进 7难道每次都要调用 xargs 把 PID 传递给 kill? 答案是否定的: 1kill -9 `ps -aux | grep firefox | awk '&#123;print $2&#125;'` 改进 8没错, 命令依然有点长, 换成 pgrep. 1kill -9 `pgrep firefox` 改进 9 - pkill看到 pkill 想到了什么? 没错 pgrep 和 kill! pkill = pgrep + kill. 1pkill -9 firefox 改进 10 - killallkillall 和 pkill 是相似的, 不过如果给出的进程名不完整, killall 会报错. pkill 或者 pgrep 只要给出进程名的一部分就可以终止进程. 1killall -9 firefox 总结12345678ps -ef # *ps -aux # *pgrep firefox # *pidof firefox-binpgrep firefox | xargs kill -9kill -9 `pgrep firefox` # *pkill -9 firefox # *killall -9 firefox # * 参考http://blog.csdn.net/a351945755/article/details/20210087]]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取脚本所在绝对目录]]></title>
    <url>%2F2016-10%2FGet-Absolute-Path-Of-Script.html</url>
    <content type="text"><![CDATA[因为脚本可能是在别的目录里调用的, 甚至是在子 shell 中调用的, 所以仅 pwd 是不行的 应该用它 1curpath=$(cd `dirname $0`; pwd)]]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 下 SVN 服务器搭建]]></title>
    <url>%2F2016-10%2FBuild-Svn-Server-On-Linux.html</url>
    <content type="text"><![CDATA[本文讲解 Linux 下 SVN 服务器的搭建: 基本的单仓库 SVN 服务器, 支持 HTTP 协议的 SVN 服务器, 多仓库的 SVN 服务器。 基础安装步骤如下: yum install subversion 查看安装版本 svnserve –version 12345678910111213svnserve, 版本 1.7.14 (r1542130)编译于 Nov 20 2015, 19:25:09版权所有 (C) 2013 Apache 软件基金会.此软件包含了许多人的贡献, 请查看文件 NOTICE 以获得更多信息.Subversion 是开放源代码软件, 请参阅 http://subversion.apache.org/ 站点.下列版本库后端(FS) 模块可用:* fs_base : 模块只能操作 BDB 版本库.* fs_fs : 模块与文本文件 (FSFS) 版本库一起工作.Cyrus SASL 认证可用. 创建 svn 版本库目录 mkdir -p /var/svn/project1 创建版本库 svnadmin create /var/svn/project1 12345678[will@master-cent7-1:/var/svn/project1]$&gt;ll总用量 16drwxr-xr-x. 2 root root 51 10月 24 09:28 confdrwxr-sr-x. 6 root root 4096 10月 24 09:12 db-r--r--r--. 1 root root 2 10月 24 09:12 formatdrwxr-xr-x. 2 root root 4096 10月 24 09:12 hooksdrwxr-xr-x. 2 root root 39 10月 24 09:12 locks-rw-r--r--. 1 root root 229 10月 24 09:12 README.txt 进入 conf 目录(该 svn 版本库配置文件) cd conf/ 123authz 文件是权限控制文件passwd 是帐号密码文件svnserve.conf svn 服务配置文件 设置帐号密码, 在文件 passwd 中: 在[users]块中添加用户和密码, 格式: 帐号=密码, 如 ljc=ljc 12345678910[will@master-cent7-1:/var/svn/project1/conf]$&gt;vim passwd### This file is an example password file for svnserve.### Its format is similar to that of svnserve.conf. As shown in the### example below it contains one section labelled [users].### The name and password for each user follow, one account per line.[users]# harry = harryssecret# sally = sallyssecretljc=ljc 设置权限: 在文件 authz 添加如下代码: 1234567[groups]admin = user1, user2[/]@admin = rw # admin 用户组对当前版本库的根目录有读写权限ljc = rw* = # 其他用户权限为空 修改 svnserve.conf 文件: 打开下面的几个注释 12345anon-access = none # 匿名用户无权限auth-access = write # 授权用户可写password-db = passwd # 使用哪个文件作为账号文件authz-db = authz # 使用哪个文件作为权限文件realm = project1 # 认证空间名 启动 svn 1svnserve -d --listen-port 3690 -r /var/svn/project1 # 用 svn://192.168.125.171[:3690] 打开 /project1 or 1svnserve -d --listen-port 3690 -r /var/svn # 用 svn://192.168.125.171[:3690]/project1 打开 /project1 停止 svn 命令 1killall svnserve 高级1. 搭建支持 HTTP 协议的 svn 服务器http://www.centoscn.com/CentosServer/ftp/2015/0620/5701.html 2. 配置 svn 服务器开机启动1systemctl enable svnserve.service 3. 多仓库两个仓库12svnadmin create /var/svn/project1svnadmin create /var/svn/project2 共用一份 authz 和 passwd 文件12cd /var/svn/project1/confcp authz passwd /var/svn passwd 的填写见 6. authz 修改如下 1234567891011121314[groups]admin = user1, user2group1 = abc,cdegroup2 = cde,ljk[project1:/] # 仓库 1 与仓库 2 权限管理不同@group1 = rw # group1 用户组对 project1 项目的根目录有读写权限ljc = rw* = # 其他用户权限为空[project2:/]@group2 = rw # group2 用户组对 project2 项目的根目录有读写权限ljc = rw* = 若是多个仓库的认证权限一样, 可以这样(若指定了仓库独有的权限就会 屏蔽 这一条) 12[/] # admin 用户组对 **所有未指定独有权限的项目的根目录** 有读写权限@admin = rw svnserve.conf 文件的填写project1 的 svnserve.conf 文件 12345anon-access = none # 匿名用户无权限auth-access = write # 授权用户可写password-db = ../../passwd # 与 project2 共用一个authz-db = ../../authz # 与 project2 共用一个realm = project1 # 认证空间名 project2 的 svnserve.conf 文件 12345anon-access = none # 匿名用户无权限auth-access = write # 授权用户可写password-db = ../../passwd # 与 project1 共用一个authz-db = ../../authz # 与 project1 共用一个realm = project2 # 认证空间名 #### 启动 svn 1svnserve -d --listen-port 3690 -r /var/svn/ 用 svn://192.168.125.171[:3690]/project1 访问 /project1 用 svn://192.168.125.171[:3690]/project2 访问 /project2]]></content>
      <categories>
        <category>版本管理</category>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 配置服务随开机启动]]></title>
    <url>%2F2016-10%2FService-Start-On-Boot.html</url>
    <content type="text"><![CDATA[法 1: systemctl enable svnserve.service, 参考 CentOS-7-Systemctl-Command 法 2: 写到 /etc/rc.d/rc.local 中]]></content>
      <categories>
        <category>Linux</category>
        <category>运维</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ps 命令]]></title>
    <url>%2F2016-10%2FCommand-Usage-ps.html</url>
    <content type="text"><![CDATA[ps 命令是 Process Status 的缩写ps 命令列出的是执行 ps 命令的时刻的进程的快照, 相关命令: top | htop | pstree linux 上进程有 5 种状态 R TASK_RUNNING 可执行状态 正在运行或在运行队列中等待 - Linux 中 “运行” 和 “就绪” 都是 R S TASK_INTERRUPTIBLE 可中断的睡眠状态 在等待某事件的发生而被挂起 进程列表中的绝大多数进程都处于 S 状态 (除非机器的负载很高) D TASK_UNINTERRUPTIBLE 不可中断的睡眠状态 不可中断指的并不是 CPU 不响应外部硬件的中断, 即是响应中断的 而是指进程不响应异步信号, 即 kill -9 杀不死! D 状态总是非常短暂的 (一般是 IO), 通过 ps 命令基本上不可能捕捉到. T TASK_STOPPED or TASK_TRACED 暂停状态或跟踪状态 (如 gdb 中的断点跟踪) 进程收到 SIGSTOP, SIGSTP, SIGTIN, SIGTOU 信号后停止运行 向 TASK_STOPPED 状态的进程发送一个 SIGCONT 信号, 可以让其从 TASK_STOPPED 状态恢复到 TASK_RUNNING 状态 处于 TASK_TRACED 状态的进程不能响应 SIGCONT 信号而被唤醒, 只能等到调试进程通过 ptrace 系统调用执行 PTRACE_CONT、PTRACE_DETACH 等操作, 或调试进程退出, 被调试的进程才能恢复 TASK_RUNNING 状态 Z ASK_DEAD – EXIT_ZOMBIE 退出状态 进程在退出的过程中, 处于 TASK_DEAD 状态, 但进程描述符存在, 直到父进程调用 wait4() 系统调用后释放 进程成为僵尸进程 常用参数ps 命令支持三种语法格式 UNIX 风格, 选项可以组合在一起, 选项前 有 &quot;-&quot; 连字符 BSD 风格, 选项可以组合在一起, 选项前 没有 &quot;-&quot; 连字符 GNU 风格的长选项, 选项前 有两个 &quot;-&quot; 连字符 可以混用 (?), 但可能有冲突, 我一般使用 UNIX 风格的 ps 命令 ps [-aAcdefHjlmNVwy] [acefhgLnrsSTuvxX] [-C&lt;指令名称&gt;] [-g&lt;群组名称&gt;][-G&lt;群组号码&gt;] [-p&lt;程序识别码&gt;][p&lt;程序识别码&gt;] [-u&lt;用户识别码&gt;][-U&lt;用户识别码&gt;][U&lt;用户名称&gt;] [-s&lt;阶段作业&gt;] [-t&lt;终端机编号&gt;][t&lt;终端机号码&gt;] [-&lt;程序识别码&gt;] [–group&lt;群组名称&gt;][-Group&lt;群组识别码&gt;] [–pid&lt;程序识别码&gt;][–user&lt;用户名称&gt;][–User&lt;用户识别码&gt;] [–sid&lt;阶段作业&gt;][–tty&lt;终端机编码&gt;][–rows&lt;显示列数&gt;] [–cols&lt;每行字符数&gt;][–columns&lt;每列字符数&gt;][–cumulative][–deselect][–forest][–headers][–no-headers][–lines&lt;显示列数&gt;][–width&lt;每列字符数&gt;] [–info][–version][–help] -a 显示所有终端机下执行的程序, 除了阶段作业领导者之外. a 显示现行终端机下的所有程序, 包括其他用户的程序. -e = -A 显示所有进程 -f 显示 UID,PPIP,C 与 STIME 栏位. f = -H 以树状结构显示进程的层次 -g 或 -G &lt;群组识别码&gt; 列出属于该群组的程序的状况, 也可使用群组名称来指定. -C &lt;指令名称&gt; 列出指定指令的程序的状况 -j 或 j 采用工作控制的格式显示程序状况. -l 或 l 详细格式 -o 用户自定义格式. -p 123 指定 pid -u abc 指定 username u 以用户为主的格式来显示程序状况. x 显示所有程序, 不以终端机来区分. 示例 常用命令 1234ps -efps -lAps auxps axjf 12345678[will@willpc ~]$ ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 126804 7604 ? Ss 11月19 1:09 /usr/lib/systemd/systemd --switched-root --system --deserialize 21root 2 0.0 0.0 0 0 ? S 11月19 0:00 [kthreadd]root 3 0.1 0.0 0 0 ? S 11月19 31:42 [ksoftirqd/0]root 7 0.0 0.0 0 0 ? S 11月19 0:00 [migration/0]root 8 0.0 0.0 0 0 ? S 11月19 0:00 [rcu_bh]root 9 0.0 0.0 0 0 ? S 11月19 0:00 [rcuob/0] 根据 CPU 使用来升序排序 1ps -aux --sort -pcpu | less 根据 内存使用 来升序排序 1ps -aux --sort -pmem | less 我们也可以将它们合并到一个命令, 并通过管道显示前 10 个结果: 1ps -aux --sort -pcpu,+pmem | head -n 10 根据线程来过滤进程 如果我们想知道特定进程的线程, 可以使用 -L 参数, 后面加上特定的 PID. 1ps -L 1213 有时候我们希望以树形结构显示进程, 可以使用 -axjf 参数. 1ps -axjf 或者可以使用另一个命令. 1pstree 结果往往过长, 一般配合 more/less 和 grep 使用 ps 输出的 Head 标头 解释 USER: 该 process 属于哪个使用者 PID : 该 process 的号码 %CPU: 该 process 使用掉的 CPU 资源百分比 %MEM: 该 process 所占用的物理内存百分比 VSZ : 该 process 使用掉的虚拟内存量 (Kbytes) RSS : 该 process 占用的固定的内存量 (Kbytes) TTY : 该 process 是在那个终端机上面运作, 若与终端机无关, 则显示 , 另外, tty1-tty6 是本机上面的登入者程序, 若为 pts/0 等等的, 则表示为由网络连接进主机的程序 STAT: 该程序目前的状态, 主要的状态有: D 不可中断 Uninterruptible(usually IO) R 正在运行, 或在队列中的进程 S 处于休眠状态 T 停止或被追踪 Z 僵尸进程 W 进入内存交换 (从内核 2.6 开始无效) X 死掉的进程 &lt; 高优先级 n 低优先级 s 包含子进程 + 位于后台的进程组 制定格式输出来查看进程状态1ps -eo user,stat..,cmd 参数 -e 显示所有进程信息, -o 参数控制输出. pid, user 和 args 参数显示 PID, 运行应用的用户和运行参数. user 用户名 uid 用户号 pid 进程号 ppid 父进程号 size 内存大小, Kbytes 字节. vsize 总虚拟内存大小, bytes 字节 (包含 code+data+stack) share 总共享页数 nice 进程优先级 (缺省为 0, 最大为 -20) priority(pri) 内核调度优先级 pmem 进程分享的物理内存数的百分比 trs 程序执行代码驻留大小 rss 进程使用的总物理内存数, Kbytes 字节 time 进程执行起到现在总的 CPU 暂用时间 stat 进程状态 cmd(args) 执行命令的简单格式 例子: 查看当前系统进程的 uid,pid,stat,pri, 以 uid 号排序. 1ps -eo pid,stat,pri,uid –sort uid 查看当前系统进程的 user,pid,stat,rss,args, 以 rss 排序. 1ps -eo user,pid,stat,rss,args –sort rss 使用 PS 实时监控进程状态ps 命令会显示你系统当前的进程状态, 但是这个结果是静态的. 当有一种情况, 我们需要像上面第四点中提到的通过 CPU 和内存的使用率来筛选进程, 并且我们希望结果能够每秒刷新一次. 为此, 我们可以将 ps 命令和 watch 命令结合起来. 1watch -n 1 'ps -aux --sort -pmem, -pcpu' 如果输出太长, 我们也可以限制它, 比如前 20 条, 我们可以使用 head 命令来做到. 1watch -n 1 'ps -aux --sort -pmem, -pcpu | head 20' 这里的动态查看并不像 top 或者 htop 命令一样美观. 但是使用 ps 的好处是你能够定义显示的字段, 还可以输出到文件方便日后分析. 举个例子, 如果你只需要看名为’pungki’用户的信息, 你可以使用下面的命令: 1watch -n 1 'ps -aux -U pungki u --sort -pmem, -pcpu | head 20']]></content>
      <categories>
        <category>Linux</category>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 systemctl 使用]]></title>
    <url>%2F2016-10%2FCentOS-7-Systemctl-Command.html</url>
    <content type="text"><![CDATA[简介systemctl 是系统服务管理器命令, 它实际上将 service 和 chkconfig 这两个命令组合到一起 systemctl 命令格式1systemctl [OPTIONS...] &#123;COMMAND&#125;... systemctl 示例以 nginx 服务为例, 实现停止、启动、重启的动作如下: 1234567systemctl stop nginx.service # 停止systemctl start nginx.service # 启动systemctl restart nginx.service # 重启systemctl status nginx.service # 检查服务状态systemctl enable nginx.service # 使服务开机启动systemctl disable nginx.service # 取消服务开机启动systemctl list -units --type=service # 查看所有已启动的服务 彻底关闭服务 123systemctl status nginx.servicesystemctl stop nginx.servicesystemctl disable nginx.service systemctl 与 旧指令 chkconfig service 对比 任务 旧指令 新指令 使某服务自动启动 chkconfig –level 3 httpd on systemctl enable httpd.service 使某服务不自动启动 chkconfig –level 3 httpd off systemctl disable httpd.service 检查服务状态 service httpd status systemctl status httpd.service (服务详细信息) systemctl is-active httpd.service (仅显示是否 Active) 显示所有已启动的服务 chkconfig –list systemctl list-units –type=service 启动某服务 service httpd start systemctl start httpd.service 停止某服务 service httpd stop systemctl stop httpd.service 重启某服务 service httpd restart systemctl restart httpd.service]]></content>
      <categories>
        <category>Linux</category>
        <category>运维</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 防火墙 firewalld 使用]]></title>
    <url>%2F2016-10%2FCentOS-7-Firewalld-Usage.html</url>
    <content type="text"><![CDATA[简介Centos 7 使用 firewalld 代替了原来的 iptables, 最大的好处有两个: 支持动态更新, 不用重启服务 就是加入了防火墙的 “zone” 概念 firewalld 有图形界面和工具界面, 图形界面请参照官方文档 firewalld 的字符界面管理工具是 firewall-cmd firewalld 默认配置文件有两个: /usr/lib/firewalld/ (系统配置, 尽量不要修改) 和 /etc/firewalld/ (用户配置地址) zone 概念硬件防火墙默认一般有三个区, firewalld 引入了这一概念, 系统默认存在以下区域 (根据文档自己理解, 如果有误请指正): drop: 默认丢弃所有包 block: 拒绝所有外部连接, 允许内部发起的连接 public: 指定外部连接可以进入 external: 这个不太明白, 功能上和上面相同, 允许指定的外部连接 dmz: 和硬件防火墙一样, 受限制的公共连接可以进入 work: 工作区, 概念和 workgoup 一样, 也是指定的外部连接允许 home: 类似家庭组 internal: 信任所有连接 对防火墙不算太熟悉, 还不太明白。 public、external、dmz、work、home 从功能上都需要自定义允许连接, 具体使用上的区别还需高人指点 管理 firewalld12345678yum install firewalld firewall-config # 安装 firewalldsystemctl enable firewalld # 设置开机自启动systemctl disable firewalld # 取消开机自启动systemctl start firewalld # 启动systemctl stop firewalld # 停止systemctl restart firewalld # 重启防火墙systemctl status firewalld # 查看防火墙状态 查看 firewalld 信息12345firewall-cmd --state # 状态firewall-cmd --help # 帮助firewall-cmd --version # 版本firewall-cmd --get-active-zones # 区域信息firewall-cmd --get-zone-of-interface=eth0 # 指定接口所属区域 配置 firewalld1234567891011121314151617181920212223242526# 配置完成后都要更新防火墙规则firewall-cmd --reload # 更新防火墙规则firewall-cmd --complete-reload # 两者的区别就是第一个无需断开连接, 就是 firewalld 特性之一动态添加规则, 第二个需要断开连接, 类似重启服务# 打开端口 -- 这个最常用firewall-cmd --zone=public --list-ports # 查看区域 public 所有打开的端口firewall-cmd --zone=public --add-port=8080/tcp # 加入一个端口到区域 publicfirewall-cmd --zone=public --add-port=8080/tcp --permanent # 永久生效firewall-cmd --zone=public --add-source=192.168.125.0/24 # 加入源 IP 到区域 publicfirewall-cmd --zone=public --add-source=192.168.125.0/24 --permanent # 永久生效firewall-cmd --zone=public --add-interface=eth0 # 将接口添加到区域, 默认接口都在 publicfirewall-cmd --zone=public --add-interface=eth0 --permanent # 永久生效firewall-cmd --set-default-zone=public # 设置默认接口区域, 立即生效无需重启firewall-cmd --panic-on # 拒绝所有包firewall-cmd --panic-off # 取消拒绝状态firewall-cmd --query-panic # 查看是否拒绝# 打开一个服务, 类似于将端口可视化, 服务需要在配置文件中添加, /etc/firewalld 目录下有 services 文件夹, 这个不详细说了, 详情参考文档firewall-cmd --zone=work --add-service=smtp# 移除服务firewall-cmd --zone=work --remove-service=smtp]]></content>
      <categories>
        <category>Linux</category>
        <category>运维</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>CentOS</tag>
        <tag>防火墙</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 软件仓库 国内镜像源]]></title>
    <url>%2F2016-10%2FCentOS-Repo-Mirror.html</url>
    <content type="text"><![CDATA[163 源: http://mirrors.163.com/.help/centos.html 阿里源: http://mirrors.aliyun.com/help/centos – 可能要去掉 aliyuncs.com 的 url]]></content>
      <categories>
        <category>Linux</category>
        <category>运维</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 配置 之 英文个人目录]]></title>
    <url>%2F2016-10%2FChinese-Linux-With-English-Directory.html</url>
    <content type="text"><![CDATA[Step 1终端: 12export LANG=en_USxdg-user-dirs-gtk-update 这时会弹出一个配置界面, 提示是否将中文目录切换为英文目录, 确定. 系统会删除没有内容的中文目录, 而有内容的目录会保持. 并创建 8 个相应的英文目录: “Desktop”, “Download”, “Templates”, “Public”, “Documents”, “Music”, “Pictures”, “Videos”. 这时常用中文目录已经变成英文目录. Step 212export LANG=zh_CN.UTF-8xdg-user-dirs-gtk-update 这次选否, 且选上 不再提示.]]></content>
      <categories>
        <category>Linux</category>
        <category>配置</category>
      </categories>
      <tags>
        <tag>Linux 配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 配置 之 获取 Root 权限]]></title>
    <url>%2F2016-05%2FObtain-Root-Permission.html</url>
    <content type="text"><![CDATA[本文是用于找回虚拟机中的 Linux Root 密码。实体机应该也可以使用。:) 方法1. 启动虚拟机2. 在开启过程中一直按 ESC 进入如下界面 如果没有进入该界面而是直接开机了, 请重启并不断尝试按 ESC 直至出现该界面 进入该界面后选中第二项 Advanced Options for Ubuntu 按下 Enter 3. 进入如下界面 选中第二项, 也就是带有 recovery mode 的选项, 注意, 千万别回车! 按下 e 键进入编辑模式 4. 进入如下界面后, 注意红圈内容 将其删掉并修改为: quiet splash rw init=/bin/bash 5. 按 ctrl+x 或者 F10 执行进入如下界面: 6. 输入代码: passwd root 按下 Enter 执行, 然后提示你输入新密码: 输入两次后修改成功, 然后重启就好了. 用 guest 用户登录, 在终端输入 su - 可切临时换到 root 账户. 至此 root 密码就是你设置的新密码, 完成. 题外话另外 修改好密码后, guest 不能 sudo, 因为 guest 用户没有 sudo 权限, 需要作如下修改: 以 guest 用户登录: 打开终端 输入 su - 切换到 root 用户: 输入: vim /etc/sudoers 在 %sudo ALL=(ALL:ALL) ALL 的下一行添加 guest ALL=(ALL) ALL, 保存退出 这样 guest 就有 sudo 权限了.]]></content>
      <categories>
        <category>Linux</category>
        <category>配置</category>
      </categories>
      <tags>
        <tag>Linux 配置</tag>
        <tag>Root</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[公共 DNS 解析]]></title>
    <url>%2F2016-05%2FPublic-DNS.html</url>
    <content type="text"><![CDATA[国外 DNS 服务器地址 Google DNS (8.8.8.8, 8.8.4.4) OpenDNS (208.67.222.222, 208.67.220.220) OpenDNS Family (208.67.222.123, 208.67.220.123) V2EX DNS (199.91.73.222, 178.79.131.110) Comodo Secure (8.26.56.26, 8.20.247.20) UltraDNS (156.154.70.1, 156.154.71.1) Norton ConnectSafe (199.85.126.10, 199.85.127.10) 国内 DNS 服务器地址 OneDNS (112.124.47.27) OpenerDNS (42.120.21.30) aliDNS (223.5.5.5, 223.6.6.6) 114DNS (114.114.114.114, 114.114.115.115) 114DNS 安全版 (114.114.114.119, 114.114.115.119) 114DNS 家庭版 (114.114.114.110, 114.114.115.110) 阿里 DNS (223.5.5.5, 223.6.6.6) 百度 DNS (180.76.76.76)]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
</search>